{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import spacy.cli\n",
    "import json\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import re\n",
    "import matplotlib.dates as mdates\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from community import community_louvain\n",
    "import matplotlib.cm as cm\n",
    "from networkx.algorithms.community.quality import modularity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_full_content = pd.read_csv('news_articles_election_candidates_full_content_cleaned.csv')\n",
    "\n",
    "# Load the spaCy English model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "except OSError:\n",
    "    spacy.cli.download(\"en_core_web_trf\")\n",
    "    nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Initialize NLTK's VADER sentiment analyzer\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Load manual politician data from JSON file\n",
    "def load_manual_politicians(file_path=\"manual_politicians.json\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}. Using an empty dictionary.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load manual politicians\n",
    "manual_politicians = load_manual_politicians()\n",
    "\n",
    "# Function to retrieve U.S. politicians and their aliases\n",
    "def get_current_us_congress_members():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = '''\n",
    "    SELECT DISTINCT ?person ?personLabel ?aliasLabel WHERE {\n",
    "      VALUES ?position { wd:Q13217683 wd:Q13218630 }  # U.S. Senator and Representative\n",
    "      ?person p:P39 ?positionStatement.\n",
    "      ?positionStatement ps:P39 ?position;\n",
    "                         pq:P580 ?startTime.\n",
    "      FILTER NOT EXISTS { ?positionStatement pq:P582 ?endTime. }  # Position with no end time\n",
    "      OPTIONAL { ?person skos:altLabel ?aliasLabel FILTER (LANG(?aliasLabel) = \"en\") }\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    '''\n",
    "    headers = {'Accept': 'application/sparql-results+json'}\n",
    "    response = requests.get(url, params={'query': query}, headers=headers, timeout=60)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "\n",
    "    politician_aliases = {}\n",
    "    for item in data['results']['bindings']:\n",
    "        canonical_name = item['personLabel']['value']\n",
    "        alias = item.get('aliasLabel', {}).get('value')\n",
    "        if canonical_name not in politician_aliases:\n",
    "            politician_aliases[canonical_name] = set()\n",
    "            politician_aliases[canonical_name].add(canonical_name)\n",
    "        if alias:\n",
    "            politician_aliases[canonical_name].add(alias)\n",
    "    return politician_aliases\n",
    "\n",
    "# Fetch U.S. politician aliases\n",
    "politician_aliases_raw = get_current_us_congress_members()\n",
    "\n",
    "# Merge with manual politicians\n",
    "politician_aliases_raw.update(manual_politicians)\n",
    "\n",
    "# Build alias-to-canonical mapping\n",
    "alias_to_canonical = {}\n",
    "for canonical_name, aliases in politician_aliases_raw.items():\n",
    "    for alias in aliases:\n",
    "        alias_to_canonical[alias.lower()] = canonical_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading saved article meantions and policitian meantions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the saved files\n",
    "article_mentions_file = \"article_mentions.pkl\"\n",
    "politician_mentions_file = \"politician_mentions.pkl\"\n",
    "\n",
    "# Load the saved mentions\n",
    "with open(article_mentions_file, \"rb\") as f:\n",
    "    article_mentions = pickle.load(f)\n",
    "with open(politician_mentions_file, \"rb\") as f:\n",
    "    politician_mentions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save sentiment scores\n",
    "sentiment_scores_file = \"sentiment_scores.pkl\"\n",
    "\n",
    "# Check if sentiment scores are already saved\n",
    "if os.path.exists(sentiment_scores_file):\n",
    "    # Load the saved sentiment scores\n",
    "    with open(sentiment_scores_file, \"rb\") as f:\n",
    "        sentiment_scores = pickle.load(f)\n",
    "    data_full_content['sentiment_scores'] = data_full_content.index.map(sentiment_scores)\n",
    "    print(\"Loaded sentiment scores from file.\")\n",
    "else:\n",
    "    # Compute sentiment scores\n",
    "    print(\"Computing sentiment scores...\")\n",
    "    sentiment_scores = {}\n",
    "    for idx, content in data_full_content['full_content'].items():\n",
    "        if isinstance(content, str):\n",
    "            sentiment_scores[idx] = sia.polarity_scores(content)\n",
    "        else:\n",
    "            sentiment_scores[idx] = {'compound': 0}\n",
    "    # Save the sentiment scores\n",
    "    with open(sentiment_scores_file, \"wb\") as f:\n",
    "        pickle.dump(sentiment_scores, f)\n",
    "    data_full_content['sentiment_scores'] = data_full_content.index.map(sentiment_scores)\n",
    "    print(\"Sentiment scores computed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading graph for network 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the graph\n",
    "graph_file = \"politician_mention_graph.graphml\"\n",
    "\n",
    "# Check if the graph is already saved\n",
    "if os.path.exists(graph_file):\n",
    "    # Load the saved graph\n",
    "    G = nx.read_graphml(graph_file)\n",
    "    print(\"Loaded graph G from file.\")\n",
    "else:\n",
    "    # Create the network graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes with attributes (politicians and articles)\n",
    "    for politician, articles in politician_mentions.items():\n",
    "        G.add_node(politician, articles=list(articles))\n",
    "    \n",
    "    # Add edges based on co-mentions in articles\n",
    "    for article_idx, mentioned_politicians in article_mentions.items():\n",
    "        mentioned_politicians = list(mentioned_politicians)\n",
    "        for i in range(len(mentioned_politicians)):\n",
    "            for j in range(i + 1, len(mentioned_politicians)):\n",
    "                p1, p2 = mentioned_politicians[i], mentioned_politicians[j]\n",
    "                if G.has_edge(p1, p2):\n",
    "                    G[p1][p2]['weight'] += 1\n",
    "                else:\n",
    "                    G.add_edge(p1, p2, weight=1)\n",
    "                    \n",
    "     # Convert list attributes to strings for GraphML compatibility\n",
    "    for node, data in G.nodes(data=True):\n",
    "        data['articles'] = \",\".join(map(str, data['articles']))\n",
    "    # Save the graph\n",
    "    nx.write_graphml(G, graph_file)\n",
    "    print(\"Graph G created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output graph information\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(\"Sample nodes with attributes:\")\n",
    "for node, attrs in list(G.nodes(data=True))[:5]:\n",
    "    print(f\"{node}: {attrs}\")\n",
    "print(\"Sample edges with weights:\")\n",
    "for u, v, attrs in list(G.edges(data=True))[:5]:\n",
    "    print(f\"{u} - {v}: {attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading bipartite graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save the bipartite graph\n",
    "bipartite_graph_file = \"bipartite_graph.graphml\"\n",
    "\n",
    "# Check if the bipartite graph is already saved\n",
    "if os.path.exists(bipartite_graph_file):\n",
    "    # Load the saved bipartite graph\n",
    "    B = nx.read_graphml(bipartite_graph_file)\n",
    "    print(\"Loaded bipartite graph B from file.\")\n",
    "else:\n",
    "    # Create a Bipartite Graph\n",
    "    B = nx.Graph()\n",
    "\n",
    "    # Add meta-nodes for media categories\n",
    "    B.add_node(\"Democratic Media\", bipartite=0, type=\"Media\")\n",
    "    B.add_node(\"Republican Media\", bipartite=0, type=\"Media\")\n",
    "\n",
    "    # Initialize dictionaries to track mentions and sentiments\n",
    "    democratic_mentions = defaultdict(list)\n",
    "    republican_mentions = defaultdict(list)\n",
    "\n",
    "    # Define media outlet categories\n",
    "    democratic_outlets = [\n",
    "        \"cnn\", \"msnbc\", \"new york times\", \"nyt\", \"npr\", \"guardian\", \"huffpost\", \"huffington post\",\n",
    "        \"slate\", \"vox\", \"politico\", \"buzzfeed\", \"buzzfeed news\"\n",
    "    ]\n",
    "    republican_outlets = [\n",
    "        \"fox news\", \"fox\", \"breitbart\", \"wall street journal\", \"wsj\", \"washington times\",\n",
    "        \"national review\", \"daily caller\", \"blaze\", \"newsmax\", \"federalist\", \"oann\",\n",
    "        \"one america news network\"\n",
    "    ]\n",
    "\n",
    "    # Function to map source names to media categories\n",
    "    def get_media_category(source):\n",
    "        source = str(source).strip().lower()\n",
    "        # Remove common prefixes and suffixes\n",
    "        source = re.sub(r'^(the|www\\.)\\s+', '', source)\n",
    "        source = re.sub(r'\\.com$', '', source)\n",
    "        source = source.replace('-', ' ')\n",
    "        source = source.replace('.', ' ')\n",
    "        source = source.replace(',', '')\n",
    "        \n",
    "        # Check for democratic media\n",
    "        for pattern in democratic_outlets:\n",
    "            if pattern in source:\n",
    "                return 'Democratic Media'\n",
    "        \n",
    "        # Check for republican media\n",
    "        for pattern in republican_outlets:\n",
    "            if pattern in source:\n",
    "                return 'Republican Media'\n",
    "        \n",
    "        return None  # Unknown media category\n",
    "\n",
    "    # Add a new column for media category\n",
    "    data_full_content['media_category'] = data_full_content['source'].apply(get_media_category)\n",
    "\n",
    "    # Process articles\n",
    "    for idx, row in tqdm(data_full_content.iterrows(), total=data_full_content.shape[0], desc=\"Building Bipartite Graph\"):\n",
    "        category = row.get('media_category')\n",
    "        if not category:\n",
    "            continue\n",
    "\n",
    "        sentiment = row['sentiment_scores']['compound'] if 'sentiment_scores' in row else 0\n",
    "        mentioned_politicians = article_mentions.get(idx, set())\n",
    "\n",
    "        for politician in mentioned_politicians:\n",
    "            if category == \"Democratic Media\":\n",
    "                democratic_mentions[politician].append(sentiment)\n",
    "            elif category == \"Republican Media\":\n",
    "                republican_mentions[politician].append(sentiment)\n",
    "\n",
    "    # Add politician nodes and edges to the bipartite graph\n",
    "    all_politicians = set(democratic_mentions.keys()).union(republican_mentions.keys())\n",
    "\n",
    "    for politician in all_politicians:\n",
    "        # Add politician node\n",
    "        B.add_node(politician, bipartite=1, type=\"Politician\")\n",
    "\n",
    "        # Calculate edge attributes for Democratic media\n",
    "        if politician in democratic_mentions:\n",
    "            mention_count = len(democratic_mentions[politician])\n",
    "            avg_sentiment = sum(democratic_mentions[politician]) / mention_count\n",
    "            B.add_edge(\"Democratic Media\", politician, weight=mention_count, sentiment=avg_sentiment)\n",
    "\n",
    "        # Calculate edge attributes for Republican media\n",
    "        if politician in republican_mentions:\n",
    "            mention_count = len(republican_mentions[politician])\n",
    "            avg_sentiment = sum(republican_mentions[politician]) / mention_count\n",
    "            B.add_edge(\"Republican Media\", politician, weight=mention_count, sentiment=avg_sentiment)\n",
    "\n",
    "    # Save the bipartite graph\n",
    "    nx.write_graphml(B, bipartite_graph_file)\n",
    "    print(\"Bipartite graph B created and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output graph information\n",
    "print(f\"Number of nodes: {B.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {B.number_of_edges()}\")\n",
    "print(\"Sample nodes with attributes:\")\n",
    "for node, attrs in list(B.nodes(data=True))[:5]:\n",
    "    print(f\"{node}: {attrs}\")\n",
    "print(\"Sample edges with weights:\")\n",
    "for u, v, attrs in list(B.edges(data=True))[:5]:\n",
    "    print(f\"{u} - {v}: {attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT transformer analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sentiment analysis function\n",
    "# Load the tokenizer and model for sentiment analysis\n",
    "# Using 'nlptown/bert-base-multilingual-uncased-sentiment' which outputs ratings from 1 (very negative) to 5 (very positive)\n",
    "tokenizer = AutoTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "\n",
    "# Move model to GPU if available for faster processing\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    # Ensure the text is a string\n",
    "    if not isinstance(text, str):\n",
    "        return None  # or you can return a default value like 3 (neutral)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        text,\n",
    "        return_tensors='pt',      # Return PyTorch tensors\n",
    "        truncation=True,          # Truncate long texts to model's max length\n",
    "        max_length=512,           # Max length for BERT\n",
    "        padding='max_length'      # Pad shorter texts\n",
    "    )\n",
    "    \n",
    "    # Move inputs to device\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the predicted class\n",
    "    logits = outputs.logits\n",
    "    probabilities = torch.softmax(logits, dim=1)\n",
    "    predicted_class = torch.argmax(probabilities, dim=1).item()\n",
    "    \n",
    "    # The model predicts ratings from 0 to 4 (0: very negative, 4: very positive)\n",
    "    # Adjust the rating to be from 1 to 5\n",
    "    sentiment_rating = predicted_class + 1\n",
    "    \n",
    "    return sentiment_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save transformer sentiment scores\n",
    "transformer_sentiment_file = \"transformer_sentiment_scores.pkl\"\n",
    "\n",
    "# Check if transformer sentiment scores are already saved\n",
    "if os.path.exists(transformer_sentiment_file):\n",
    "    # Load the saved sentiment scores\n",
    "    with open(transformer_sentiment_file, \"rb\") as f:\n",
    "        transformer_sentiment_scores = pickle.load(f)\n",
    "    data_full_content['transformer_sentiment'] = data_full_content.index.map(transformer_sentiment_scores)\n",
    "    print(\"Loaded transformer sentiment scores from file.\")\n",
    "else:\n",
    "    # Compute transformer sentiment scores\n",
    "    print(\"Computing transformer sentiment scores...\")\n",
    "    transformer_sentiment_scores = {}\n",
    "    for idx, content in tqdm(data_full_content['full_content'].items(), desc=\"Sentiment Analysis\"):\n",
    "        score = get_sentiment_score(content)\n",
    "        transformer_sentiment_scores[idx] = score\n",
    "    # Save the sentiment scores\n",
    "    with open(transformer_sentiment_file, \"wb\") as f:\n",
    "        pickle.dump(transformer_sentiment_scores, f)\n",
    "    data_full_content['transformer_sentiment'] = data_full_content.index.map(transformer_sentiment_scores)\n",
    "    print(\"Transformer sentiment scores computed and saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'published_at' column is in datetime format\n",
    "data_full_content['published_at'] = pd.to_datetime(data_full_content['published_at'], errors='coerce')\n",
    "\n",
    "# Drop rows with NaT in 'published_at' (optional, depending on your data)\n",
    "data_full_content = data_full_content.dropna(subset=['published_at'])\n",
    "\n",
    "# Extract the date from the timestamp\n",
    "data_full_content['date'] = data_full_content['published_at'].dt.date\n",
    "\n",
    "## Aggregate Sentiments Over Time\n",
    "# Compute the average sentiment per day for each politician\n",
    "sentiment_over_time = []\n",
    "\n",
    "for politician, indices in politician_mentions.items():\n",
    "    # Subset data for articles mentioning the politician\n",
    "    politician_data = data_full_content.loc[list(indices)]\n",
    "    \n",
    "    if not politician_data.empty:\n",
    "        # Ensure 'sentiment_score' exists in data_full_content\n",
    "        if 'sentiment_score' not in politician_data.columns:\n",
    "            # Compute 'sentiment_score' if not present\n",
    "            from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "            sia = SentimentIntensityAnalyzer()\n",
    "            politician_data['sentiment_score'] = politician_data['full_content'].apply(\n",
    "                lambda content: sia.polarity_scores(content)['compound'] if isinstance(content, str) else 0\n",
    "            )\n",
    "        \n",
    "        # Group by date and compute average sentiment\n",
    "        daily_sentiment = (\n",
    "            politician_data.groupby('date')['sentiment_score']\n",
    "            .mean()\n",
    "            .reset_index()\n",
    "        )\n",
    "        daily_sentiment['normalized_sentiment'] = (daily_sentiment['sentiment_score'] - 0) / 1  # Adjust if needed\n",
    "        daily_sentiment['politician'] = politician\n",
    "        sentiment_over_time.append(daily_sentiment)\n",
    "\n",
    "# Combine all politicians' sentiment data into a single DataFrame\n",
    "if sentiment_over_time:\n",
    "    sentiment_over_time_df = pd.concat(sentiment_over_time, ignore_index=True)\n",
    "else:\n",
    "    sentiment_over_time_df = pd.DataFrame()\n",
    "    print(\"No sentiment data available.\")\n",
    "\n",
    "# Save the DataFrame\n",
    "sentiment_over_time_df.to_pickle(sentiment_over_time_file)\n",
    "print(\"Sentiment over time data computed and saved.\")\n",
    "\n",
    "# Now proceed with visualization\n",
    "# Display the resulting DataFrame\n",
    "print(sentiment_over_time_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualize Sentiment Trends\n",
    "\n",
    "# Select top politicians to visualize\n",
    "top_politicians = ['Joe Biden', 'Donald Trump', 'Kamala Harris']\n",
    "\n",
    "# Plot sentiment over time for each politician\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for politician in top_politicians:\n",
    "    data = sentiment_over_time_df[sentiment_over_time_df['politician'] == politician]\n",
    "    if not data.empty:\n",
    "        plt.plot(data['date'], data['normalized_sentiment'], label=politician)\n",
    "    else:\n",
    "        print(f\"No data available for {politician}\")\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title('Sentiment Over Time for Selected Politicians')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Sentiment')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Improve date formatting\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=7))  # Every 7 days\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key events plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify Key Events and Correlate with Sentiment Changes\n",
    "\n",
    "# Define key events\n",
    "# Source: https://en.wikipedia.org/wiki/Timeline_of_the_2024_United_States_presidential_election\n",
    "key_events = {\n",
    "    # '2024-09-10': 'ABC presidential debate',\n",
    "    '2024-10-27': 'Trump Rally at Madison Square Garden',\n",
    "    # '2024-10-01': 'CBS vice presidential debate',\n",
    "    '2024-10-25': 'Washington Post announces they will not endorse',\n",
    "    '2024-11-01': 'Green Party endorses Kamala Harris',\n",
    "    # Add more events as needed...\n",
    "}\n",
    "\n",
    "# Plot sentiment over time with key events\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for politician in top_politicians:\n",
    "    data = sentiment_over_time_df[sentiment_over_time_df['politician'] == politician]\n",
    "    if not data.empty:\n",
    "        plt.plot(data['date'], data['normalized_sentiment'], label=politician)\n",
    "    else:\n",
    "        print(f\"No data available for {politician}\")\n",
    "\n",
    "# Add vertical lines for key events\n",
    "for date_str, event in key_events.items():\n",
    "    event_date = pd.to_datetime(date_str).date()\n",
    "    plt.axvline(x=event_date, color='grey', linestyle='--', alpha=0.7)\n",
    "    plt.text(event_date, plt.ylim()[1], event, rotation=90, verticalalignment='top', fontsize=8)\n",
    "\n",
    "# Formatting the plot\n",
    "plt.title('Sentiment Over Time with Key Events')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Normalized Sentiment')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Improve date formatting\n",
    "plt.gca().xaxis.set_major_locator(mdates.DayLocator(interval=7))  # Every 7 days\n",
    "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Decetion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment profiles for politicians "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sentiment profiles\n",
    "sentiment_profiles = {}\n",
    "\n",
    "for politician in all_politicians:\n",
    "    profile = {}\n",
    "    # Extract sentiment scores from edges\n",
    "    if B.has_edge(\"Democratic Media\", politician):\n",
    "        profile[\"Democratic Media\"] = B[\"Democratic Media\"][politician][\"sentiment\"]\n",
    "    else:\n",
    "        profile[\"Democratic Media\"] = 0  # No connection\n",
    "\n",
    "    if B.has_edge(\"Republican Media\", politician):\n",
    "        profile[\"Republican Media\"] = B[\"Republican Media\"][politician][\"sentiment\"]\n",
    "    else:\n",
    "        profile[\"Republican Media\"] = 0  # No connection\n",
    "\n",
    "    sentiment_profiles[politician] = profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create similarity based graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment profiles to a DataFrame\n",
    "profiles_df = pd.DataFrame.from_dict(sentiment_profiles, orient=\"index\").fillna(0)\n",
    "\n",
    "# Compute similarity between sentiment profiles\n",
    "similarity_matrix = cosine_similarity(profiles_df)\n",
    "\n",
    "# Create a graph where edge weights are similarity scores\n",
    "P = nx.Graph()\n",
    "politicians = profiles_df.index.tolist()\n",
    "\n",
    "for i, pol1 in enumerate(politicians):\n",
    "    for j, pol2 in enumerate(politicians):\n",
    "        if i != j and similarity_matrix[i, j] > 0:  # Ignore self-loops\n",
    "            P.add_edge(pol1, pol2, weight=similarity_matrix[i, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply community detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect communities\n",
    "partition = community_louvain.best_partition(P, weight=\"weight\")\n",
    "\n",
    "# Group nodes by community\n",
    "communities = defaultdict(list)\n",
    "for politician, community_id in partition.items():\n",
    "    communities[community_id].append(politician)\n",
    "\n",
    "# Print community results\n",
    "for community_id, members in communities.items():\n",
    "    print(f\"Community {community_id}: {len(members)} members\")\n",
    "    print(f\"Politicians: {members}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse and visualize communities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for community_id, members in communities.items():\n",
    "    print(f\"Community {community_id}:\")\n",
    "    community_profiles = profiles_df.loc[members].mean()\n",
    "    print(f\"Average Sentiment Profile:\")\n",
    "    print(community_profiles)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign colors based on community\n",
    "cmap = cm.get_cmap(\"viridis\", max(partition.values()) + 1)\n",
    "node_colors = [cmap(partition[node]) for node in P.nodes()]\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(P, seed=42)  # Use a spring layout for visualization\n",
    "nx.draw(P, pos, with_labels=True, node_color=node_colors, node_size=300, font_size=8, alpha=0.8)\n",
    "plt.title(\"Community Network Based on Sentiment Similarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format communities for modularity calculation\n",
    "formatted_communities = [members for members in communities.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute modularity score\n",
    "modularity_score = modularity(P, formatted_communities, weight=\"weight\")\n",
    "\n",
    "print(f\"Modularity Score: {modularity_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
