{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from newspaper import Article\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NewsAPI Client\n",
    "api_key = \"\"  # Replace with API key\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"news_articles_election_candidates_expanded.csv\"\n",
    "full_content_file = \"news_articles_election_candidates_full_content.csv\"\n",
    "\n",
    "# Function to fetch full article content using newspaper3k\n",
    "def fetch_full_content(article_url):\n",
    "    try:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text  # Return the full article text\n",
    "    except Exception as e:\n",
    "        return None  # Return None if there is an error\n",
    "\n",
    "# Set new date range\n",
    "start_date = datetime.date(2024, 10, 12)  # Continue from where the previous script left off\n",
    "end_date = datetime.date(2024, 8, 1)  # Adjust end date as needed for backward collection\n",
    "\n",
    "# Prepare to store results\n",
    "articles_data = []\n",
    "\n",
    "# Expanded search queries\n",
    "queries = [\n",
    "    \"2024 Presidential election\",\n",
    "    \"US election AND (Donald Trump OR Kamala Harris)\",\n",
    "    \"Biden administration AND 2024 election\",\n",
    "    \"(Donald Trump OR Trump) AND 2024 election\",\n",
    "    \"(Kamala Harris OR Harris) AND 2024 election\",\n",
    "    \"Campaign financing AND 2024 election\",\n",
    "    \"Voter turnout AND 2024 election\",\n",
    "    \"Presidential debate AND 2024 election\",\n",
    "    \"(Donald Trump OR Trump) AND rally AND 2024\",\n",
    "    \"(Kamala Harris OR Harris) AND speech AND 2024\",\n",
    "    \"(Donald Trump OR Kamala Harris) AND 2024 election\",\n",
    "    \"(Trump OR Harris) AND campaign AND 2024 election\"\n",
    "]\n",
    "\n",
    "# Track API request count to avoid exceeding limits\n",
    "request_count = 0\n",
    "max_requests = 100  # Free-tier daily API limit\n",
    "\n",
    "# Load existing expanded and full content CSV files\n",
    "existing_expanded_data = pd.read_csv(input_file) if os.path.exists(input_file) else pd.DataFrame()\n",
    "existing_full_content_data = pd.read_csv(full_content_file) if os.path.exists(full_content_file) else pd.DataFrame()\n",
    "\n",
    "# Track URLs to avoid duplicates\n",
    "existing_urls = set(existing_expanded_data[\"url\"]) if not existing_expanded_data.empty else set()\n",
    "processed_urls = set(existing_full_content_data[\"url\"]) if not existing_full_content_data.empty else set()\n",
    "\n",
    "# Fetch new articles from NewsAPI\n",
    "current_date = start_date\n",
    "while current_date >= end_date:\n",
    "    # Convert date to string for API\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching articles for {date_str}...\")\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            # Check if API limit is reached\n",
    "            if request_count >= max_requests:\n",
    "                print(\"Reached API limit for the day. Exiting script.\")\n",
    "                sys.exit()\n",
    "\n",
    "            # Fetch articles for the current query and date\n",
    "            response = newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=date_str,\n",
    "                to=date_str,\n",
    "                language=\"en\",\n",
    "                sort_by=\"relevancy\",  # Fetch relevant articles\n",
    "                page_size=100  # Max articles per API call\n",
    "            )\n",
    "\n",
    "            # Increment request count\n",
    "            request_count += 1\n",
    "\n",
    "            if response.get('status') != 'ok':\n",
    "                print(f\"API error: {response.get('message')}\")\n",
    "                sys.exit()\n",
    "\n",
    "            # Process the articles\n",
    "            if response.get('articles'):\n",
    "                for article in response['articles']:\n",
    "                    # Only add new articles that are not already saved\n",
    "                    if article['url'] not in existing_urls:\n",
    "                        articles_data.append({\n",
    "                            \"query\": query,  # Include the query used for tracking\n",
    "                            \"source\": article['source']['name'],\n",
    "                            \"author\": article['author'],\n",
    "                            \"title\": article['title'],\n",
    "                            \"description\": article['description'],\n",
    "                            \"url\": article['url'],\n",
    "                            \"published_at\": article['publishedAt'],\n",
    "                            \"content\": article['content']\n",
    "                        })\n",
    "                        # Add the URL to the set of existing URLs\n",
    "                        existing_urls.add(article['url'])\n",
    "        except Exception as e:\n",
    "            # Log the error to a file\n",
    "            with open(\"error_log.txt\", \"a\") as log_file:\n",
    "                log_file.write(f\"Error fetching articles for {query} on {date_str}: {e}\\n\")\n",
    "            print(f\"Error fetching articles for {query} on {date_str}: {e}\")\n",
    "    \n",
    "    # Move to the previous day\n",
    "    current_date -= datetime.timedelta(days=1)\n",
    "\n",
    "    # Avoid hitting API limits by adding a small delay between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save new articles to the expanded CSV file\n",
    "if articles_data:\n",
    "    new_data_df = pd.DataFrame(articles_data)\n",
    "    new_data_df.to_csv(input_file, mode='a', header=not os.path.exists(input_file), index=False)\n",
    "    print(f\"Appended {len(new_data_df)} new articles to '{input_file}'.\")\n",
    "\n",
    "# Extract full content for new articles only\n",
    "new_urls = {article[\"url\"] for article in articles_data}  # URLs of newly fetched articles\n",
    "urls_to_process = new_urls - processed_urls  # Exclude already processed URLs\n",
    "\n",
    "if urls_to_process:\n",
    "    full_content_data = []\n",
    "    for url in urls_to_process:\n",
    "        print(f\"Fetching full content for {url}...\")\n",
    "        full_content = fetch_full_content(url)\n",
    "        if full_content:\n",
    "            full_content_data.append({\n",
    "                \"url\": url,\n",
    "                \"full_content\": full_content\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to fetch full content for {url}\")\n",
    "\n",
    "    # Append full content to the full content CSV file\n",
    "    if full_content_data:\n",
    "        full_content_df = pd.DataFrame(full_content_data)\n",
    "        full_content_df.to_csv(full_content_file, mode='a', header=not os.path.exists(full_content_file), index=False)\n",
    "        print(f\"Appended {len(full_content_df)} new full content articles to '{full_content_file}'.\")\n",
    "else:\n",
    "    print(\"No new articles to process for full content.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9610 entries, 0 to 9609\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   query         9610 non-null   object\n",
      " 1   source        9610 non-null   object\n",
      " 2   author        9093 non-null   object\n",
      " 3   title         9607 non-null   object\n",
      " 4   description   9598 non-null   object\n",
      " 5   url           9610 non-null   object\n",
      " 6   published_at  9610 non-null   object\n",
      " 7   content       9610 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 600.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'news_articles_election_candidates_expanded.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9610 entries, 0 to 9609\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   query         9610 non-null   object\n",
      " 1   source        9610 non-null   object\n",
      " 2   author        9093 non-null   object\n",
      " 3   title         9607 non-null   object\n",
      " 4   description   9598 non-null   object\n",
      " 5   url           9610 non-null   object\n",
      " 6   published_at  9610 non-null   object\n",
      " 7   content       9610 non-null   object\n",
      " 8   full_content  7556 non-null   object\n",
      "dtypes: object(9)\n",
      "memory usage: 675.8+ KB\n"
     ]
    }
   ],
   "source": [
    "file_path = 'news_articles_election_candidates_full_content.csv'\n",
    "data_full_content = pd.read_csv(file_path)\n",
    "\n",
    "data_full_content.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove entries with missing full content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_full_content = pd.read_csv('news_articles_election_candidates_full_content_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>source</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>published_at</th>\n",
       "      <th>content</th>\n",
       "      <th>full_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024 Presidential election</td>\n",
       "      <td>Wired</td>\n",
       "      <td>Lily Hay Newman, Tess Owen</td>\n",
       "      <td>Russia Is Going All Out on Election Day Interf...</td>\n",
       "      <td>Along with other foreign influence operations—...</td>\n",
       "      <td>https://www.wired.com/story/russia-election-di...</td>\n",
       "      <td>2024-11-05T21:04:35Z</td>\n",
       "      <td>As the 2024 US presidential election comes to ...</td>\n",
       "      <td>As the 2024 US presidential election comes to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024 Presidential election</td>\n",
       "      <td>The Verge</td>\n",
       "      <td>Justine Calma</td>\n",
       "      <td>Apple News will let you watch election results...</td>\n",
       "      <td>On Election Day in the US, Apple News is rolli...</td>\n",
       "      <td>https://www.theverge.com/2024/11/5/24288777/el...</td>\n",
       "      <td>2024-11-05T16:34:12Z</td>\n",
       "      <td>Image: Cath Virginia / The Verge\\r\\n\\n \\n\\n Fo...</td>\n",
       "      <td>For anyone obsessively watching election resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024 Presidential election</td>\n",
       "      <td>NPR</td>\n",
       "      <td>Megan Pratz</td>\n",
       "      <td>Here's how NPR will report the 2024 election r...</td>\n",
       "      <td>NPR relies on results and race calls from The ...</td>\n",
       "      <td>https://www.npr.org/2024/11/04/g-s1-31268/2024...</td>\n",
       "      <td>2024-11-05T10:00:00Z</td>\n",
       "      <td>Voters have been voting, ballots will be count...</td>\n",
       "      <td>Here's how NPR will report the 2024 election r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024 Presidential election</td>\n",
       "      <td>Business Insider</td>\n",
       "      <td>insider@insider.com (John L. Dorman,Kelsey Vla...</td>\n",
       "      <td>The 2024 presidential election may come down t...</td>\n",
       "      <td>Election results in the swing states of Arizon...</td>\n",
       "      <td>https://www.businessinsider.com/what-are-2024-...</td>\n",
       "      <td>2024-11-05T22:13:18Z</td>\n",
       "      <td>Over the past two decades, the road to the Whi...</td>\n",
       "      <td>On Election Day 2024, both parties are eyeing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024 Presidential election</td>\n",
       "      <td>CNET</td>\n",
       "      <td>Thomas Kika</td>\n",
       "      <td>How Do I Keep Track of Official Election Resul...</td>\n",
       "      <td>An avalanche of Election Day 2024 coverage is ...</td>\n",
       "      <td>https://www.cnet.com/tech/services-and-softwar...</td>\n",
       "      <td>2024-11-05T17:45:00Z</td>\n",
       "      <td>As if the 2024 election reporting could be any...</td>\n",
       "      <td>All anyone is talking about today is the presi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        query            source  \\\n",
       "0  2024 Presidential election             Wired   \n",
       "1  2024 Presidential election         The Verge   \n",
       "2  2024 Presidential election               NPR   \n",
       "3  2024 Presidential election  Business Insider   \n",
       "4  2024 Presidential election              CNET   \n",
       "\n",
       "                                              author  \\\n",
       "0                         Lily Hay Newman, Tess Owen   \n",
       "1                                      Justine Calma   \n",
       "2                                        Megan Pratz   \n",
       "3  insider@insider.com (John L. Dorman,Kelsey Vla...   \n",
       "4                                        Thomas Kika   \n",
       "\n",
       "                                               title  \\\n",
       "0  Russia Is Going All Out on Election Day Interf...   \n",
       "1  Apple News will let you watch election results...   \n",
       "2  Here's how NPR will report the 2024 election r...   \n",
       "3  The 2024 presidential election may come down t...   \n",
       "4  How Do I Keep Track of Official Election Resul...   \n",
       "\n",
       "                                         description  \\\n",
       "0  Along with other foreign influence operations—...   \n",
       "1  On Election Day in the US, Apple News is rolli...   \n",
       "2  NPR relies on results and race calls from The ...   \n",
       "3  Election results in the swing states of Arizon...   \n",
       "4  An avalanche of Election Day 2024 coverage is ...   \n",
       "\n",
       "                                                 url          published_at  \\\n",
       "0  https://www.wired.com/story/russia-election-di...  2024-11-05T21:04:35Z   \n",
       "1  https://www.theverge.com/2024/11/5/24288777/el...  2024-11-05T16:34:12Z   \n",
       "2  https://www.npr.org/2024/11/04/g-s1-31268/2024...  2024-11-05T10:00:00Z   \n",
       "3  https://www.businessinsider.com/what-are-2024-...  2024-11-05T22:13:18Z   \n",
       "4  https://www.cnet.com/tech/services-and-softwar...  2024-11-05T17:45:00Z   \n",
       "\n",
       "                                             content  \\\n",
       "0  As the 2024 US presidential election comes to ...   \n",
       "1  Image: Cath Virginia / The Verge\\r\\n\\n \\n\\n Fo...   \n",
       "2  Voters have been voting, ballots will be count...   \n",
       "3  Over the past two decades, the road to the Whi...   \n",
       "4  As if the 2024 election reporting could be any...   \n",
       "\n",
       "                                        full_content  \n",
       "0  As the 2024 US presidential election comes to ...  \n",
       "1  For anyone obsessively watching election resul...  \n",
       "2  Here's how NPR will report the 2024 election r...  \n",
       "3  On Election Day 2024, both parties are eyeing ...  \n",
       "4  All anyone is talking about today is the presi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows (articles): 7556\n",
      "Number of variables (columns): 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows (articles):\", data_full_content.shape[0])\n",
    "print(\"Number of variables (columns):\", data_full_content.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique media outlets: 454\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique media outlets\n",
    "unique_media_outlets = data_full_content['source'].nunique()\n",
    "\n",
    "# Display the number of unique media outlets\n",
    "print(f\"Number of unique media outlets: {unique_media_outlets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 842\n",
      "Number of edges: 34386\n",
      "Sample nodes: [('Donald Trump', {'articles': [0, 1, 3, 4, 5, 6, 7, 9, 10, 12, 13, 15, 17, 18, 19, 20, 21, 24, 25, 27, 29, 30, 33, 34, 38, 40, 42, 43, 45, 49, 50, 51, 52, 53, 55, 57, 58, 64, 65, 67, 70, 71, 74, 75, 76, 77, 78, 82, 83, 84, 86, 91, 92, 93, 94, 95, 96, 97, 98, 99]}), ('Hillary Clinton', {'articles': [0, 33, 34, 3, 7, 12, 17, 49, 19, 82, 21, 92, 95]}), ('Clinton', {'articles': [0, 58, 3]}), ('Brad Raffensperger', {'articles': [0]}), ('Cait Conley', {'articles': [0]})]\n",
      "Sample edges: [('Donald Trump', 'Brad Raffensperger', {'weight': 1}), ('Donald Trump', 'Tim Walz', {'weight': 7}), ('Donald Trump', 'Hillary Clinton', {'weight': 13}), ('Donald Trump', 'Adrian Fontes', {'weight': 2}), ('Donald Trump', 'Cait Conley', {'weight': 1})]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.7.3\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.7.3/en_core_web_trf-3.7.3-py3-none-any.whl (457.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from en-core-web-trf==3.7.3) (3.7.2)\n",
      "Requirement already satisfied: spacy-curated-transformers<0.3.0,>=0.2.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from en-core-web-trf==3.7.3) (0.2.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.0.6)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (72.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (1.26.4)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2.5.1)\n",
      "Requirement already satisfied: regex>=2022 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.9.11)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.1.4)\n",
      "Requirement already satisfied: filelock in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.16.1)\n",
      "Requirement already satisfied: networkx in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (3.3)\n",
      "Requirement already satisfied: fsspec in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<0.3.0,>=0.2.0->en-core-web-trf==3.7.3) (1.3.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (8.1.7)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-trf==3.7.3) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages/thinc/shims/pytorch.py:253: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n",
      "/Users/thomasschioler/anaconda3/envs/02805SG/lib/python3.11/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import spacy.cli\n",
    "\n",
    "# Download the en_core_web_trf model\n",
    "spacy.cli.download(\"en_core_web_trf\")\n",
    "\n",
    "# Process only the first 100 articles for testing\n",
    "data_sample = data_full_content.head(100)\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Collect authors' names to exclude them from entities\n",
    "authors_set = set()\n",
    "for author in data_sample['author'].dropna():\n",
    "    # Some articles may have multiple authors separated by commas\n",
    "    authors = [a.strip() for a in author.split(',')]\n",
    "    authors_set.update(authors)\n",
    "\n",
    "# Lowercase authors' names for consistent comparison\n",
    "authors_set = {author.lower() for author in authors_set}\n",
    "\n",
    "# Function to retrieve U.S. politicians and their aliases\n",
    "def get_current_us_congress_members():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = '''\n",
    "    SELECT DISTINCT ?person ?personLabel ?aliasLabel WHERE {\n",
    "      VALUES ?position { wd:Q13217683 wd:Q13218630 }  # U.S. Senator and Representative\n",
    "      ?person p:P39 ?positionStatement.\n",
    "      ?positionStatement ps:P39 ?position;\n",
    "                         pq:P580 ?startTime.\n",
    "      FILTER NOT EXISTS { ?positionStatement pq:P582 ?endTime. }  # Position with no end time\n",
    "      OPTIONAL { ?person skos:altLabel ?aliasLabel FILTER (LANG(?aliasLabel) = \"en\") }\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    '''\n",
    "    headers = {'Accept': 'application/sparql-results+json'}\n",
    "    response = requests.get(url, params={'query': query}, headers=headers, timeout=60)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "\n",
    "    politician_aliases = {}\n",
    "    for item in data['results']['bindings']:\n",
    "        canonical_name = item['personLabel']['value']\n",
    "        alias = item.get('aliasLabel', {}).get('value')\n",
    "        if canonical_name not in politician_aliases:\n",
    "            politician_aliases[canonical_name] = set()\n",
    "            politician_aliases[canonical_name].add(canonical_name)\n",
    "        if alias:\n",
    "            politician_aliases[canonical_name].add(alias)\n",
    "    return politician_aliases\n",
    "\n",
    "\n",
    "# Retrieve the U.S. politicians and their aliases\n",
    "politician_aliases_raw = get_current_us_congress_members()\n",
    "\n",
    "additional_politicians = {\n",
    "    \"Joe Biden\": {\"Joe Biden\", \"Joseph Biden\", \"Biden\", \"President Biden\"},\n",
    "    \"Donald Trump\": {\"Donald Trump\", \"Trump\", \"President Trump\"},\n",
    "    \"Kamala Harris\": {\"Kamala Harris\", \"Harris\", \"Vice President Harris\"},\n",
    "    # Add other candidates and relevant figures\n",
    "}\n",
    "\n",
    "politician_aliases_raw.update(additional_politicians)\n",
    "\n",
    "# Build the alias_to_canonical mapping\n",
    "alias_to_canonical = {}\n",
    "for canonical_name, aliases in politician_aliases_raw.items():\n",
    "    for alias in aliases:\n",
    "        alias_to_canonical[alias.lower()] = canonical_name\n",
    "\n",
    "# Initialize mappings for mentions\n",
    "article_mentions = defaultdict(set)  # Maps article index to mentioned politicians\n",
    "politician_mentions = defaultdict(set)  # Maps politician to articles they're mentioned in\n",
    "\n",
    "# Perform NER and normalize entity names\n",
    "# Process articles\n",
    "for idx, row in data_sample.iterrows():\n",
    "    content = row['full_content']\n",
    "    doc = nlp(content)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entity_name = ent.text.strip()\n",
    "            entity_name_lower = entity_name.lower()\n",
    "            # Exclude authors\n",
    "            if entity_name_lower in authors_set:\n",
    "                continue\n",
    "            # Map entity name to canonical politician name\n",
    "            canonical_name = alias_to_canonical.get(entity_name_lower)\n",
    "            if canonical_name:\n",
    "                article_mentions[idx].add(canonical_name)\n",
    "                politician_mentions[canonical_name].add(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 14\n",
      "Number of edges: 36\n",
      "Sample nodes with attributes:\n",
      "Donald Trump: {'articles': [0, 1, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 33, 34, 38, 40, 42, 43, 45, 48, 49, 50, 51, 52, 53, 55, 57, 58, 64, 65, 67, 69, 70, 71, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 99]}\n",
      "Kamala Harris: {'articles': [1, 3, 4, 5, 6, 7, 9, 10, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 30, 33, 34, 35, 38, 40, 42, 45, 49, 50, 51, 52, 53, 57, 58, 64, 65, 67, 70, 74, 75, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 99]}\n",
      "Joe Biden: {'articles': [3, 5, 7, 9, 10, 11, 12, 17, 18, 19, 21, 24, 28, 33, 42, 45, 49, 74, 82, 92, 94, 96, 97]}\n",
      "Ruben Gallego: {'articles': [3]}\n",
      "Dina Titus: {'articles': [3]}\n",
      "Sample edges with weights:\n",
      "Donald Trump - Kamala Harris: {'weight': 55}\n",
      "Donald Trump - Ruben Gallego: {'weight': 1}\n",
      "Donald Trump - Steven Horsford: {'weight': 1}\n",
      "Donald Trump - Dina Titus: {'weight': 1}\n",
      "Donald Trump - Joe Biden: {'weight': 22}\n"
     ]
    }
   ],
   "source": [
    "# Create the network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes (politicians and articles)\n",
    "for politician, articles in politician_mentions.items():\n",
    "    G.add_node(politician, articles=list(articles))\n",
    "\n",
    "# Add edges based on co-mentions in articles\n",
    "for article_idx, mentioned_politicians in article_mentions.items():\n",
    "    mentioned_politicians = list(mentioned_politicians)\n",
    "    for i in range(len(mentioned_politicians)):\n",
    "        for j in range(i + 1, len(mentioned_politicians)):\n",
    "            p1, p2 = mentioned_politicians[i], mentioned_politicians[j]\n",
    "            if G.has_edge(p1, p2):\n",
    "                G[p1][p2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(p1, p2, weight=1)\n",
    "\n",
    "# Output graph information\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(\"Sample nodes with attributes:\")\n",
    "for node, attrs in list(G.nodes(data=True))[:5]:\n",
    "    print(f\"{node}: {attrs}\")\n",
    "print(\"Sample edges with weights:\")\n",
    "for u, v, attrs in list(G.edges(data=True))[:5]:\n",
    "    print(f\"{u} - {v}: {attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02805SG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
