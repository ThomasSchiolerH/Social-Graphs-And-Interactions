{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from newspaper import Article\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NewsAPI Client\n",
    "api_key = \"\"  # Replace with API key\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"news_articles_election_candidates_expanded.csv\"\n",
    "full_content_file = \"news_articles_election_candidates_full_content.csv\"\n",
    "\n",
    "# Function to fetch full article content using newspaper3k\n",
    "def fetch_full_content(article_url):\n",
    "    try:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text  # Return the full article text\n",
    "    except Exception as e:\n",
    "        return None  # Return None if there is an error\n",
    "\n",
    "# Set new date range\n",
    "start_date = datetime.date(2024, 10, 12)  # Continue from where the previous script left off\n",
    "end_date = datetime.date(2024, 8, 1)  # Adjust end date as needed for backward collection\n",
    "\n",
    "# Prepare to store results\n",
    "articles_data = []\n",
    "\n",
    "# Expanded search queries\n",
    "queries = [\n",
    "    \"2024 Presidential election\",\n",
    "    \"US election AND (Donald Trump OR Kamala Harris)\",\n",
    "    \"Biden administration AND 2024 election\",\n",
    "    \"(Donald Trump OR Trump) AND 2024 election\",\n",
    "    \"(Kamala Harris OR Harris) AND 2024 election\",\n",
    "    \"Campaign financing AND 2024 election\",\n",
    "    \"Voter turnout AND 2024 election\",\n",
    "    \"Presidential debate AND 2024 election\",\n",
    "    \"(Donald Trump OR Trump) AND rally AND 2024\",\n",
    "    \"(Kamala Harris OR Harris) AND speech AND 2024\",\n",
    "    \"(Donald Trump OR Kamala Harris) AND 2024 election\",\n",
    "    \"(Trump OR Harris) AND campaign AND 2024 election\"\n",
    "]\n",
    "\n",
    "# Track API request count to avoid exceeding limits\n",
    "request_count = 0\n",
    "max_requests = 100  # Free-tier daily API limit\n",
    "\n",
    "# Load existing expanded and full content CSV files\n",
    "existing_expanded_data = pd.read_csv(input_file) if os.path.exists(input_file) else pd.DataFrame()\n",
    "existing_full_content_data = pd.read_csv(full_content_file) if os.path.exists(full_content_file) else pd.DataFrame()\n",
    "\n",
    "# Track URLs to avoid duplicates\n",
    "existing_urls = set(existing_expanded_data[\"url\"]) if not existing_expanded_data.empty else set()\n",
    "processed_urls = set(existing_full_content_data[\"url\"]) if not existing_full_content_data.empty else set()\n",
    "\n",
    "# Fetch new articles from NewsAPI\n",
    "current_date = start_date\n",
    "while current_date >= end_date:\n",
    "    # Convert date to string for API\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching articles for {date_str}...\")\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            # Check if API limit is reached\n",
    "            if request_count >= max_requests:\n",
    "                print(\"Reached API limit for the day. Exiting script.\")\n",
    "                sys.exit()\n",
    "\n",
    "            # Fetch articles for the current query and date\n",
    "            response = newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=date_str,\n",
    "                to=date_str,\n",
    "                language=\"en\",\n",
    "                sort_by=\"relevancy\",  # Fetch relevant articles\n",
    "                page_size=100  # Max articles per API call\n",
    "            )\n",
    "\n",
    "            # Increment request count\n",
    "            request_count += 1\n",
    "\n",
    "            if response.get('status') != 'ok':\n",
    "                print(f\"API error: {response.get('message')}\")\n",
    "                sys.exit()\n",
    "\n",
    "            # Process the articles\n",
    "            if response.get('articles'):\n",
    "                for article in response['articles']:\n",
    "                    # Only add new articles that are not already saved\n",
    "                    if article['url'] not in existing_urls:\n",
    "                        articles_data.append({\n",
    "                            \"query\": query,  # Include the query used for tracking\n",
    "                            \"source\": article['source']['name'],\n",
    "                            \"author\": article['author'],\n",
    "                            \"title\": article['title'],\n",
    "                            \"description\": article['description'],\n",
    "                            \"url\": article['url'],\n",
    "                            \"published_at\": article['publishedAt'],\n",
    "                            \"content\": article['content']\n",
    "                        })\n",
    "                        # Add the URL to the set of existing URLs\n",
    "                        existing_urls.add(article['url'])\n",
    "        except Exception as e:\n",
    "            # Log the error to a file\n",
    "            with open(\"error_log.txt\", \"a\") as log_file:\n",
    "                log_file.write(f\"Error fetching articles for {query} on {date_str}: {e}\\n\")\n",
    "            print(f\"Error fetching articles for {query} on {date_str}: {e}\")\n",
    "    \n",
    "    # Move to the previous day\n",
    "    current_date -= datetime.timedelta(days=1)\n",
    "\n",
    "    # Avoid hitting API limits by adding a small delay between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save new articles to the expanded CSV file\n",
    "if articles_data:\n",
    "    new_data_df = pd.DataFrame(articles_data)\n",
    "    new_data_df.to_csv(input_file, mode='a', header=not os.path.exists(input_file), index=False)\n",
    "    print(f\"Appended {len(new_data_df)} new articles to '{input_file}'.\")\n",
    "\n",
    "# Extract full content for new articles only\n",
    "new_urls = {article[\"url\"] for article in articles_data}  # URLs of newly fetched articles\n",
    "urls_to_process = new_urls - processed_urls  # Exclude already processed URLs\n",
    "\n",
    "if urls_to_process:\n",
    "    full_content_data = []\n",
    "    for url in urls_to_process:\n",
    "        print(f\"Fetching full content for {url}...\")\n",
    "        full_content = fetch_full_content(url)\n",
    "        if full_content:\n",
    "            full_content_data.append({\n",
    "                \"url\": url,\n",
    "                \"full_content\": full_content\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to fetch full content for {url}\")\n",
    "\n",
    "    # Append full content to the full content CSV file\n",
    "    if full_content_data:\n",
    "        full_content_df = pd.DataFrame(full_content_data)\n",
    "        full_content_df.to_csv(full_content_file, mode='a', header=not os.path.exists(full_content_file), index=False)\n",
    "        print(f\"Appended {len(full_content_df)} new full content articles to '{full_content_file}'.\")\n",
    "else:\n",
    "    print(\"No new articles to process for full content.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove entries with missing full content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_full_content = pd.read_csv('news_articles_election_candidates_full_content_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from en-core-web-trf==3.8.0) (0.3.0)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.4.0)\n",
      "Requirement already satisfied: regex>=2022 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.9.11)\n",
      "Requirement already satisfied: filelock in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages/thinc/shims/pytorch.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(filelike, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual politician aliases added:\n",
      "Joe Biden: Joe Biden, Joseph Biden, Biden, President Biden\n",
      "Donald Trump: Donald Trump, Trump, President Trump\n",
      "Kamala Harris: Kamala Harris, Harris, Vice President Harris\n",
      "Barack Obama: Barack Obama, Obama, President Obama\n",
      "Hillary Clinton: Hillary Clinton, Hillary, Secretary Clinton, Clinton\n",
      "Bill Clinton: Bill Clinton, President Clinton\n",
      "George W. Bush: George W. Bush, Bush, President Bush\n",
      "Nancy Pelosi: Nancy Pelosi, Pelosi, Speaker Pelosi\n",
      "Mitch McConnell: Mitch McConnell, McConnell, Senator McConnell\n",
      "Bernie Sanders: Bernie Sanders, Sanders, Senator Sanders\n",
      "Elizabeth Warren: Elizabeth Warren, Warren, Senator Warren\n",
      "Kevin McCarthy: Kevin McCarthy, McCarthy, Speaker McCarthy\n",
      "Alexandria Ocasio-Cortez: Alexandria Ocasio-Cortez, AOC, Ocasio-Cortez\n",
      "Ted Cruz: Ted Cruz, Cruz, Senator Cruz\n",
      "Marco Rubio: Marco Rubio, Rubio, Senator Rubio\n",
      "Chuck Schumer: Chuck Schumer, Schumer, Senator Schumer, Majority Leader Schumer\n",
      "Ron DeSantis: Ron DeSantis, DeSantis, Governor DeSantis\n",
      "Gavin Newsom: Gavin Newsom, Newsom, Governor Newsom\n",
      "Nikki Haley: Nikki Haley, Haley, Ambassador Haley\n",
      "Mike Pence: Mike Pence, Pence, Vice President Pence\n",
      "Pete Buttigieg: Pete Buttigieg, Buttigieg, Secretary Buttigieg, Mayor Pete\n",
      "Amy Klobuchar: Amy Klobuchar, Klobuchar, Senator Klobuchar\n",
      "Cory Booker: Cory Booker, Booker, Senator Booker\n",
      "Mitt Romney: Mitt Romney, Romney, Senator Romney\n",
      "Rashida Tlaib: Rashida Tlaib, Tlaib, Representative Tlaib\n",
      "Ilhan Omar: Ilhan Omar, Omar, Representative Omar\n",
      "Ayanna Pressley: Ayanna Pressley, Pressley, Representative Pressley\n",
      "Lauren Boebert: Lauren Boebert, Boebert, Representative Boebert\n",
      "Marjorie Taylor Greene: Marjorie Taylor Greene, MTG, Greene, Representative Greene\n",
      "Josh Hawley: Josh Hawley, Hawley, Senator Hawley\n",
      "Kyrsten Sinema: Kyrsten Sinema, Sinema, Senator Sinema\n",
      "Joe Manchin: Joe Manchin, Manchin, Senator Manchin\n",
      "Liz Cheney: Liz Cheney, Cheney, Representative Cheney\n",
      "Adam Schiff: Adam Schiff, Schiff, Representative Schiff\n",
      "Jerry Nadler: Jerry Nadler, Nadler, Representative Nadler\n",
      "Lauren Underwood: Lauren Underwood, Underwood, Representative Underwood\n",
      "Katie Porter: Katie Porter, Porter, Representative Porter\n",
      "Tim Scott: Tim Scott, Scott, Senator Scott\n",
      "Lindsey Graham: Lindsey Graham, Graham, Senator Graham\n",
      "Tom Cotton: Tom Cotton, Cotton, Senator Cotton\n",
      "Andrew Yang: Andrew Yang, Yang\n",
      "Beto O'Rourke: Beto O'Rourke, Beto, O'Rourke\n",
      "Sarah Palin: Sarah Palin, Palin, Governor Palin\n",
      "Gretchen Whitmer: Gretchen Whitmer, Whitmer, Governor Whitmer\n",
      "Stacey Abrams: Stacey Abrams, Abrams\n",
      "Ro Khanna: Ro Khanna, Khanna, Representative Khanna\n",
      "Elise Stefanik: Elise Stefanik, Stefanik, Representative Stefanik\n",
      "Deb Haaland: Deb Haaland, Haaland, Secretary Haaland\n",
      "John Fetterman: John Fetterman, Fetterman, Senator Fetterman\n",
      "Katie Hobbs: Katie Hobbs, Hobbs, Governor Hobbs\n",
      "J.D. Vance: J.D. Vance, Vance, Senator Vance\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import spacy.cli\n",
    "import json\n",
    "\n",
    "# Download the en_core_web_trf model\n",
    "spacy.cli.download(\"en_core_web_trf\")\n",
    "\n",
    "# Process only the first 100 articles for testing\n",
    "data_sample = data_full_content.head(100)\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Collect authors' names to exclude them from entities\n",
    "authors_set = set()\n",
    "for author in data_sample['author'].dropna():\n",
    "    # Some articles may have multiple authors separated by commas\n",
    "    authors = [a.strip() for a in author.split(',')]\n",
    "    authors_set.update(authors)\n",
    "\n",
    "# Lowercase authors' names for consistent comparison\n",
    "authors_set = {author.lower() for author in authors_set}\n",
    "\n",
    "# Function to retrieve U.S. politicians and their aliases\n",
    "def get_current_us_congress_members():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = '''\n",
    "    SELECT DISTINCT ?person ?personLabel ?aliasLabel WHERE {\n",
    "      VALUES ?position { wd:Q13217683 wd:Q13218630 }  # U.S. Senator and Representative\n",
    "      ?person p:P39 ?positionStatement.\n",
    "      ?positionStatement ps:P39 ?position;\n",
    "                         pq:P580 ?startTime.\n",
    "      FILTER NOT EXISTS { ?positionStatement pq:P582 ?endTime. }  # Position with no end time\n",
    "      OPTIONAL { ?person skos:altLabel ?aliasLabel FILTER (LANG(?aliasLabel) = \"en\") }\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    '''\n",
    "    headers = {'Accept': 'application/sparql-results+json'}\n",
    "    response = requests.get(url, params={'query': query}, headers=headers, timeout=60)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "\n",
    "    politician_aliases = {}\n",
    "    for item in data['results']['bindings']:\n",
    "        canonical_name = item['personLabel']['value']\n",
    "        alias = item.get('aliasLabel', {}).get('value')\n",
    "        if canonical_name not in politician_aliases:\n",
    "            politician_aliases[canonical_name] = set()\n",
    "            politician_aliases[canonical_name].add(canonical_name)\n",
    "        if alias:\n",
    "            politician_aliases[canonical_name].add(alias)\n",
    "    return politician_aliases\n",
    "\n",
    "\n",
    "# Retrieve the U.S. politicians and their aliases\n",
    "politician_aliases_raw = get_current_us_congress_members()\n",
    "\n",
    "# Load manual politician data from JSON file\n",
    "def load_manual_politicians(file_path=\"manual_politicians.json\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}. Using an empty dictionary.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load manual politicians\n",
    "manual_politicians = load_manual_politicians()\n",
    "\n",
    "# Add manual politicians to the alias dictionary\n",
    "politician_aliases_raw.update(manual_politicians)\n",
    "\n",
    "# Logging function to verify additions\n",
    "def log_aliases(aliases):\n",
    "    print(\"Manual politician aliases added:\")\n",
    "    for canonical_name, aliases_set in aliases.items():\n",
    "        print(f\"{canonical_name}: {', '.join(aliases_set)}\")\n",
    "\n",
    "# Log the manual politician data being added\n",
    "log_aliases(manual_politicians)\n",
    "\n",
    "\n",
    "# Build the alias_to_canonical mapping\n",
    "alias_to_canonical = {}\n",
    "for canonical_name, aliases in politician_aliases_raw.items():\n",
    "    for alias in aliases:\n",
    "        alias_to_canonical[alias.lower()] = canonical_name\n",
    "\n",
    "# Initialize mappings for mentions\n",
    "article_mentions = defaultdict(set)  # Maps article index to mentioned politicians\n",
    "politician_mentions = defaultdict(set)  # Maps politician to articles they're mentioned in\n",
    "\n",
    "# Perform NER and normalize entity names\n",
    "# Process articles\n",
    "for idx, row in data_sample.iterrows():\n",
    "    content = row['full_content']\n",
    "    doc = nlp(content)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            entity_name = ent.text.strip()\n",
    "            entity_name_lower = entity_name.lower()\n",
    "            # Exclude authors\n",
    "            if entity_name_lower in authors_set:\n",
    "                continue\n",
    "            # Map entity name to canonical politician name\n",
    "            canonical_name = alias_to_canonical.get(entity_name_lower)\n",
    "            if canonical_name:\n",
    "                article_mentions[idx].add(canonical_name)\n",
    "                politician_mentions[canonical_name].add(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cell for Setup (Run Once)\n",
    "This cell performs tasks that only need to be done once, such as downloading the spaCy model and loading manual politician data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-curated-transformers<1.0.0,>=0.2.2 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from en-core-web-trf==3.8.0) (0.3.0)\n",
      "Requirement already satisfied: curated-transformers<0.2.0,>=0.1.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.1.1)\n",
      "Requirement already satisfied: curated-tokenizers<0.1.0,>=0.0.9 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.0.9)\n",
      "Requirement already satisfied: torch>=1.12.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.4.0)\n",
      "Requirement already satisfied: regex>=2022 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.9.11)\n",
      "Requirement already satisfied: filelock in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/victorwintherlarsen/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import spacy\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Download the spaCy model (Run this only once)\n",
    "try:\n",
    "    import spacy.cli\n",
    "    spacy.cli.download(\"en_core_web_trf\")\n",
    "except Exception as e:\n",
    "    print(\"Model download skipped or already complete:\", e)\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Load manual politician data from JSON file\n",
    "def load_manual_politicians(file_path=\"manual_politicians.json\"):\n",
    "    try:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}. Using an empty dictionary.\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        return {}\n",
    "\n",
    "# Load manual politicians (only needs to be loaded once)\n",
    "manual_politicians = load_manual_politicians()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cell for Data Retrieval and Processing\n",
    "Use this cell for tasks that depend on external data or need to be re-executed when the data changes (e.g., fetching politician aliases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to retrieve U.S. politicians and their aliases\n",
    "def get_current_us_congress_members():\n",
    "    url = 'https://query.wikidata.org/sparql'\n",
    "    query = '''\n",
    "    SELECT DISTINCT ?person ?personLabel ?aliasLabel WHERE {\n",
    "      VALUES ?position { wd:Q13217683 wd:Q13218630 }  # U.S. Senator and Representative\n",
    "      ?person p:P39 ?positionStatement.\n",
    "      ?positionStatement ps:P39 ?position;\n",
    "                         pq:P580 ?startTime.\n",
    "      FILTER NOT EXISTS { ?positionStatement pq:P582 ?endTime. }  # Position with no end time\n",
    "      OPTIONAL { ?person skos:altLabel ?aliasLabel FILTER (LANG(?aliasLabel) = \"en\") }\n",
    "      SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "    }\n",
    "    '''\n",
    "    headers = {'Accept': 'application/sparql-results+json'}\n",
    "    response = requests.get(url, params={'query': query}, headers=headers, timeout=60)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"SPARQL query failed with status {response.status_code}: {response.text}\")\n",
    "    data = response.json()\n",
    "\n",
    "    politician_aliases = {}\n",
    "    for item in data['results']['bindings']:\n",
    "        canonical_name = item['personLabel']['value']\n",
    "        alias = item.get('aliasLabel', {}).get('value')\n",
    "        if canonical_name not in politician_aliases:\n",
    "            politician_aliases[canonical_name] = set()\n",
    "            politician_aliases[canonical_name].add(canonical_name)\n",
    "        if alias:\n",
    "            politician_aliases[canonical_name].add(alias)\n",
    "    return politician_aliases\n",
    "\n",
    "# Fetch U.S. politician aliases\n",
    "politician_aliases_raw = get_current_us_congress_members()\n",
    "\n",
    "# Merge with manual politicians\n",
    "politician_aliases_raw.update(manual_politicians)\n",
    "\n",
    "# Build alias-to-canonical mapping\n",
    "alias_to_canonical = {}\n",
    "for canonical_name, aliases in politician_aliases_raw.items():\n",
    "    for alias in aliases:\n",
    "        alias_to_canonical[alias.lower()] = canonical_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell for Article Processing\n",
    "This cell processes your dataset (data_sample) and performs named entity recognition (NER). Run it whenever you need to process or analyze new articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress saved at article 500\n",
      "Progress saved at article 1200\n",
      "Progress saved at article 1300\n",
      "Progress saved at article 1400\n",
      "Progress saved at article 1500\n",
      "Progress saved at article 1600\n",
      "Progress saved at article 1700\n",
      "Progress saved at article 1800\n",
      "Progress saved at article 1900\n",
      "Progress saved at article 2000\n",
      "Progress saved at article 2100\n",
      "Progress saved at article 2200\n",
      "Progress saved at article 2300\n",
      "Progress saved at article 2400\n",
      "Progress saved at article 2500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 49\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     48\u001b[0m content \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_content\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 49\u001b[0m doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ent\u001b[38;5;241m.\u001b[39mlabel_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPERSON\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/language.py:1052\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     error_handler \u001b[38;5;241m=\u001b[39m proc\u001b[38;5;241m.\u001b[39mget_error_handler()\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1052\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcomponent_cfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE109\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy_curated_transformers/pipeline/transformer.py:242\u001b[0m, in \u001b[0;36mCuratedTransformer.predict\u001b[0;34m(self, docs)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# To ensure that the model's internal state is always consistent with the pipe's.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_model_all_layer_outputs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_layer_outputs)\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:334\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OutT:\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy_curated_transformers/models/architectures.py:651\u001b[0m, in \u001b[0;36mtransformer_model_forward\u001b[0;34m(model, docs, is_train)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformer_model_forward\u001b[39m(\n\u001b[1;32m    649\u001b[0m     model: TransformerModelT, docs: TransformerInT, is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    650\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[TransformerOutT, TransformerBackpropT]:\n\u001b[0;32m--> 651\u001b[0m     Y, backprop_layer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY):\n\u001b[1;32m    654\u001b[0m         backprop_layer(dY)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy_curated_transformers/models/with_non_ws_tokens.py:72\u001b[0m, in \u001b[0;36mwith_non_ws_tokens_forward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     70\u001b[0m inner: Model[Tok2PiecesInT, WsTokenAdapterOutT] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayers[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     71\u001b[0m tokens, ws_counts \u001b[38;5;241m=\u001b[39m _filter_tokens(X)\n\u001b[0;32m---> 72\u001b[0m Y_no_ws, backprop_no_ws \u001b[38;5;241m=\u001b[39m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Note: we modify the model outputs in-place. Since we are wrapping the\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# model, there should be no other consumers. Not sure yet if the same\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# applies to the gradient (e.g. consider summing two encoders of the\u001b[39;00m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# same width downstream.)\u001b[39;00m\n\u001b[1;32m     79\u001b[0m alignments \u001b[38;5;241m=\u001b[39m _create_alignments(model, Y_no_ws, ws_counts)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m     52\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 54\u001b[0m     Y, inc_layer_grad \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(inc_layer_grad)\n\u001b[1;32m     56\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/spacy_curated_transformers/models/with_strided_spans.py:108\u001b[0m, in \u001b[0;36mwith_strided_spans_forward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    106\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m _split_spans(spans, batch_size):\n\u001b[0;32m--> 108\u001b[0m     output, bp \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTorchTransformerInT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, TransformerModelOutput):\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m received an unexpected input of type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt can only wrap/be chained with models whose outputs are of type  \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`TransformerModelOutput` (in almost all cases, models of type `TorchTransformerModelT`)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/model.py:310\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, X, is_train)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: InT, is_train: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m    308\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/layers/pytorchwrapper.py:225\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(model, X, is_train)\u001b[0m\n\u001b[1;32m    222\u001b[0m convert_outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    224\u001b[0m Xtorch, get_dX \u001b[38;5;241m=\u001b[39m convert_inputs(model, X, is_train)\n\u001b[0;32m--> 225\u001b[0m Ytorch, torch_backprop \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtorch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m Y, get_dYtorch \u001b[38;5;241m=\u001b[39m convert_outputs(model, (X, Ytorch), is_train)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbackprop\u001b[39m(dY: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/shims/pytorch.py:97\u001b[0m, in \u001b[0;36mPyTorchShim.__call__\u001b[0;34m(self, inputs, is_train)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbegin_update(inputs)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mlambda\u001b[39;00m a: \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/thinc/shims/pytorch.py:119\u001b[0m, in \u001b[0;36mPyTorchShim.predict\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# NB: Previously this was torch.cuda.amp.autocast, passing a boolean\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# for mixed_precision. That doesn't seem to match the docs, and now\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# it raises an error when moving from the deprecated function. So\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# I've removed the argument but I'm not certain it's correct.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m, enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mixed_precision):\n\u001b[0;32m--> 119\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/curated_transformers/models/curated_transformer.py:37\u001b[0m, in \u001b[0;36mCuratedTransformer.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     29\u001b[0m     input_ids: Tensor,\n\u001b[1;32m     30\u001b[0m     attention_mask: Optional[AttentionMask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     31\u001b[0m     token_type_ids: Optional[Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     32\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PyTorchTransformerOutput:\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m    Shapes:\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03m        input_ids, attention_mask, token_type_ids - (batch, seq_len)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurated_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/curated_transformers/models/roberta/encoder.py:51\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids)\u001b[0m\n\u001b[1;32m     49\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 51\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     layer_outputs\u001b[38;5;241m.\u001b[39mappend(layer_output)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PyTorchTransformerOutput(\n\u001b[1;32m     55\u001b[0m     embedding_output\u001b[38;5;241m=\u001b[39membeddings, layer_hidden_states\u001b[38;5;241m=\u001b[39mlayer_outputs\n\u001b[1;32m     56\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/curated_transformers/models/bert/layer.py:133\u001b[0m, in \u001b[0;36mBertEncoderLayer.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    130\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_output_dropout(attn_out)\n\u001b[1;32m    131\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_output_layernorm(x \u001b[38;5;241m+\u001b[39m attn_out)\n\u001b[0;32m--> 133\u001b[0m ffn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m ffn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_output_dropout(ffn_out)\n\u001b[1;32m    135\u001b[0m ffn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn_output_layernorm(attn_out \u001b[38;5;241m+\u001b[39m ffn_out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/curated_transformers/models/bert/layer.py:100\u001b[0m, in \u001b[0;36mBertFeedForward.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     96\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124;03m    Shapes:\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;124;03m        x - (batch, seq_len, width)\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(out)\n\u001b[1;32m    102\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "# Assume data_full_content is already loaded\n",
    "data_sample = data_full_content.head(8000)\n",
    "\n",
    "# Collect authors' names to exclude them from entities\n",
    "authors_set = set()\n",
    "for author in data_sample['author'].dropna():\n",
    "    authors = [a.strip() for a in author.split(',')]\n",
    "    authors_set.update(authors)\n",
    "\n",
    "# Lowercase authors' names for consistent comparison\n",
    "authors_set = {author.lower() for author in authors_set}\n",
    "\n",
    "# Initialize mappings for mentions\n",
    "article_mentions = defaultdict(set)  # Maps article index to mentioned politicians\n",
    "politician_mentions = defaultdict(set)  # Maps politician to articles they're mentioned in\n",
    "\n",
    "# Define paths to save progress\n",
    "article_mentions_file = \"article_mentions.pkl\"\n",
    "politician_mentions_file = \"politician_mentions.pkl\"\n",
    "error_log_file = \"error_log.txt\"\n",
    "\n",
    "# Load intermediate progress if available\n",
    "if os.path.exists(article_mentions_file):\n",
    "    with open(article_mentions_file, \"rb\") as f:\n",
    "        article_mentions = pickle.load(f)\n",
    "if os.path.exists(politician_mentions_file):\n",
    "    with open(politician_mentions_file, \"rb\") as f:\n",
    "        politician_mentions = pickle.load(f)\n",
    "\n",
    "# Initialize or load error log\n",
    "error_log = []\n",
    "if os.path.exists(error_log_file):\n",
    "    with open(error_log_file, \"r\") as f:\n",
    "        error_log = f.readlines()\n",
    "\n",
    "# Process articles\n",
    "for idx, row in data_sample.iterrows():\n",
    "    try:\n",
    "        # Skip already processed articles\n",
    "        if idx in article_mentions:\n",
    "            continue\n",
    "\n",
    "        content = row['full_content']\n",
    "        doc = nlp(content)\n",
    "\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\":\n",
    "                entity_name = ent.text.strip()\n",
    "                entity_name_lower = entity_name.lower()\n",
    "                # Exclude authors\n",
    "                if entity_name_lower in authors_set:\n",
    "                    continue\n",
    "                # Map entity name to canonical politician name\n",
    "                canonical_name = alias_to_canonical.get(entity_name_lower)\n",
    "                if canonical_name:\n",
    "                    article_mentions[idx].add(canonical_name)\n",
    "                    politician_mentions[canonical_name].add(idx)\n",
    "\n",
    "        # Periodically save progress\n",
    "        if idx % 100 == 0:\n",
    "            with open(article_mentions_file, \"wb\") as f:\n",
    "                pickle.dump(article_mentions, f)\n",
    "            with open(politician_mentions_file, \"wb\") as f:\n",
    "                pickle.dump(politician_mentions, f)\n",
    "            print(f\"Progress saved at article {idx}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log the error\n",
    "        error_message = f\"Error processing article {idx}: {str(e)}\\n\"\n",
    "        error_log.append(error_message)\n",
    "        with open(error_log_file, \"a\") as f:\n",
    "            f.write(error_message)\n",
    "        print(error_message)\n",
    "\n",
    "# Final save of progress\n",
    "with open(article_mentions_file, \"wb\") as f:\n",
    "    pickle.dump(article_mentions, f)\n",
    "with open(politician_mentions_file, \"wb\") as f:\n",
    "    pickle.dump(politician_mentions, f)\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 217\n",
      "Number of edges: 2107\n",
      "Sample nodes with attributes:\n",
      "Donald Trump: {'articles': [0, 1, 3, 4, 5, 6, 7, 9, 10, 12, 13, 14, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 33, 34, 38, 40, 42, 43, 45, 48, 49, 50, 51, 52, 53, 55, 57, 58, 64, 65, 67, 69, 70, 71, 74, 75, 76, 77, 78, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 99, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 161, 162, 164, 165, 166, 167, 168, 169, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 187, 188, 189, 191, 194, 195, 196, 197, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 240, 241, 242, 243, 246, 247, 253, 254, 257, 259, 260, 261, 263, 265, 266, 269, 270, 271, 272, 273, 274, 275, 278, 279, 280, 281, 282, 284, 285, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 307, 308, 310, 311, 313, 314, 315, 316, 320, 321, 322, 323, 324, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 341, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 404, 405, 406, 407, 408, 409, 411, 412, 413, 415, 416, 418, 419, 420, 422, 423, 425, 426, 427, 429, 430, 431, 433, 434, 435, 436, 437, 438, 439, 440, 441, 444, 445, 446, 447, 448, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 465, 466, 468, 469, 470, 471, 472, 473, 474, 476, 477, 479, 480, 482, 483, 485, 487, 489, 490, 491, 492, 493, 494, 495, 496, 498, 499, 502, 504, 505, 507, 509, 510, 511, 514, 515, 517, 521, 522, 523, 525, 526, 528, 529, 531, 534, 535, 536, 537, 545, 546, 547, 548, 549, 550, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 591, 592, 593, 594, 597, 598, 599, 600, 601, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 619, 621, 622, 623, 624, 625, 626, 627, 628, 629, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 662, 663, 664, 665, 666, 667, 669, 671, 672, 673, 674, 675, 676, 677, 678, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 710, 711, 712, 713, 717, 719, 720, 721, 723, 725, 726, 730, 731, 732, 733, 734, 735, 736, 737, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 754, 756, 757, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 772, 773, 774, 775, 776, 777, 778, 779, 780, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 808, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 835, 836, 837, 838, 839, 840, 841, 842, 844, 845, 846, 847, 848, 849, 850, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 867, 868, 869, 870, 871, 872, 874, 875, 876, 877, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 894, 897, 898, 899, 900, 901, 902, 903, 904, 906, 907, 908, 909, 910, 912, 915, 918, 919, 923, 924, 926, 927, 928, 929, 930, 931, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 949, 950, 951, 952, 954, 955, 956, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1088, 1089, 1091, 1092, 1093, 1096, 1097, 1099, 1100, 1101, 1104, 1106, 1107, 1108, 1112, 1114, 1116, 1118, 1119, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1152, 1153, 1154, 1155, 1156, 1157, 1160, 1161, 1162, 1163, 1164, 1165, 1168, 1169, 1170, 1171, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1199, 1201, 1202, 1203, 1205, 1206, 1207, 1209, 1210, 1211, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1235, 1236, 1237, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1251, 1252, 1254, 1255, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1278, 1279, 1280, 1281, 1282, 1283, 1285, 1286, 1288, 1289, 1290, 1291, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1356, 1357, 1358, 1360, 1363, 1364, 1365, 1366, 1368, 1369, 1370, 1371, 1372, 1373, 1376, 1378, 1379, 1380, 1382, 1384, 1385, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1429, 1430, 1432, 1433, 1435, 1436, 1437, 1438, 1440, 1441, 1443, 1444, 1445, 1446, 1448, 1449, 1450, 1451, 1452, 1453, 1456, 1457, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1468, 1469, 1470, 1471, 1472, 1473, 1475, 1476, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1489, 1490, 1491, 1492, 1495, 1497, 1498, 1499, 1500, 1501, 1504, 1505, 1506, 1508, 1510, 1511, 1512, 1513, 1514, 1516, 1518, 1520, 1521, 1522, 1524, 1526, 1527, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1565, 1566, 1568, 1569, 1570, 1571, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1613, 1614, 1615, 1616, 1617, 1618, 1622, 1624, 1626, 1627, 1628, 1629, 1630, 1632, 1633, 1634, 1635, 1637, 1638, 1639, 1640, 1642, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1654, 1655, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1666, 1667, 1669, 1670, 1671, 1672, 1674, 1675, 1676, 1677, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1701, 1703, 1704, 1706, 1709, 1710, 1711, 1713, 1716, 1717, 1719, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1733, 1734, 1737, 1739, 1740, 1741, 1742, 1743, 1745, 1746, 1747, 1749, 1751, 1752, 1754, 1755, 1756, 1757, 1758, 1759, 1761, 1762, 1763, 1764, 1766, 1767, 1768, 1769, 1770, 1771, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1803, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1813, 1814, 1815, 1816, 1818, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1864, 1865, 1866, 1868, 1869, 1872, 1873, 1874, 1875, 1876, 1877, 1879, 1880, 1886, 1887, 1888, 1893, 1897, 1900, 1901, 1902, 1904, 1905, 1906, 1908, 1909, 1912, 1916, 1917, 1918, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1949, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1974, 1975, 1976, 1977, 1980, 1982, 1983, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2036, 2037, 2038, 2039, 2040, 2041, 2045, 2048, 2049, 2050, 2051, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2070, 2071, 2072, 2073, 2074, 2076, 2078, 2081, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2091, 2092, 2093, 2094, 2095, 2096, 2099, 2102, 2103, 2104, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2172, 2173, 2174, 2175, 2177, 2178, 2179, 2182, 2183, 2184, 2185, 2186, 2187, 2189, 2190, 2191, 2192, 2193, 2195, 2196, 2197, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2213, 2214, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2226, 2227, 2228, 2229, 2231, 2232, 2234, 2235, 2238, 2240, 2241, 2242, 2244, 2249, 2250, 2254, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2277, 2278, 2279, 2280, 2281, 2283, 2284, 2285, 2286, 2287, 2289, 2290, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2354, 2355, 2356, 2357, 2358, 2360, 2361, 2362, 2363, 2366, 2369, 2371, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2439, 2440, 2441, 2442, 2443, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2471, 2472, 2473, 2474, 2475, 2476, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2527, 2528, 2529, 2531, 2532, 2537, 2538, 2540, 2541, 2542, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2557, 2558, 2560, 2561, 2563, 2564, 2565, 2566, 2568, 2569, 2570, 2571]}\n",
      "Hillary Clinton: {'articles': [0, 1536, 3, 2051, 2053, 2563, 7, 1544, 2056, 2564, 2569, 12, 1038, 2062, 1040, 17, 1041, 19, 1554, 21, 1559, 537, 2075, 1053, 1568, 33, 34, 1572, 1573, 550, 1063, 2087, 2092, 1581, 2553, 1071, 49, 1081, 570, 58, 1599, 1604, 583, 2120, 2122, 1099, 588, 1611, 1613, 1103, 592, 82, 1106, 1624, 2139, 92, 1116, 1118, 95, 1119, 2144, 610, 1123, 1124, 1637, 2148, 105, 2155, 2158, 624, 1138, 1652, 2165, 1142, 2167, 2168, 2169, 1147, 2172, 637, 638, 639, 128, 1152, 1153, 1155, 1156, 1665, 1667, 1159, 1669, 137, 138, 139, 1673, 141, 2186, 2188, 146, 2195, 153, 154, 665, 2206, 2211, 165, 166, 168, 1709, 175, 688, 1199, 690, 1203, 1716, 1206, 2231, 701, 2241, 194, 706, 199, 711, 2177, 2250, 2254, 207, 2258, 723, 726, 2263, 217, 730, 1241, 732, 733, 1756, 736, 1249, 1252, 1764, 742, 231, 744, 1767, 746, 749, 1261, 751, 1777, 1267, 1784, 1273, 2298, 764, 2300, 2303, 2306, 774, 775, 776, 777, 1289, 267, 1801, 269, 2310, 2320, 1810, 2323, 789, 1813, 2327, 280, 792, 2330, 284, 285, 796, 800, 2339, 804, 1829, 1321, 810, 2348, 2357, 1334, 311, 1335, 1850, 320, 1351, 2380, 1357, 2381, 1872, 850, 2388, 1368, 1880, 346, 2392, 1372, 351, 352, 1375, 1376, 867, 871, 2408, 1391, 882, 1394, 884, 1396, 1398, 1906, 2423, 889, 1403, 2430, 901, 390, 1415, 1928, 2444, 909, 2447, 1939, 917, 1941, 1942, 408, 1432, 1433, 2458, 1436, 2465, 2466, 1955, 932, 2467, 423, 425, 426, 2478, 2479, 1969, 438, 1977, 956, 445, 2493, 2495, 961, 2497, 963, 968, 1480, 973, 974, 1488, 978, 1490, 2515, 2005, 982, 2006, 2516, 2517, 2521, 988, 1501, 2015, 1505, 483, 995, 1511, 1516, 1005, 2028, 496, 498, 1522, 1524, 2549, 1527, 1017, 2554, 1023]}\n",
      "Kamala Harris: {'articles': [1, 3, 4, 5, 6, 7, 9, 10, 12, 13, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 28, 30, 33, 34, 35, 38, 40, 42, 45, 49, 50, 51, 52, 53, 57, 58, 64, 65, 67, 70, 74, 75, 82, 83, 84, 85, 86, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 149, 151, 152, 153, 155, 156, 157, 158, 159, 161, 162, 164, 165, 166, 168, 169, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 186, 187, 188, 189, 191, 194, 195, 196, 197, 199, 201, 202, 203, 204, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 218, 223, 228, 232, 234, 235, 236, 240, 241, 242, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 262, 263, 265, 266, 269, 270, 271, 273, 274, 275, 278, 279, 280, 281, 282, 283, 284, 285, 288, 289, 290, 291, 293, 294, 295, 296, 297, 299, 301, 302, 303, 304, 307, 308, 310, 311, 313, 314, 315, 316, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 335, 337, 343, 344, 345, 346, 347, 349, 350, 351, 352, 353, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 368, 370, 371, 372, 373, 374, 376, 377, 382, 383, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 398, 399, 402, 404, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 446, 447, 448, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 464, 465, 466, 467, 468, 469, 471, 472, 473, 474, 476, 477, 479, 480, 482, 483, 485, 487, 489, 490, 491, 492, 493, 494, 495, 496, 498, 499, 503, 504, 505, 509, 510, 511, 514, 515, 517, 521, 522, 528, 534, 535, 537, 538, 539, 546, 547, 548, 549, 550, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 591, 592, 593, 594, 598, 599, 600, 601, 603, 604, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 618, 619, 621, 622, 623, 624, 625, 626, 628, 629, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 644, 646, 647, 648, 649, 650, 652, 653, 654, 655, 656, 657, 658, 660, 662, 663, 664, 665, 667, 669, 671, 672, 674, 675, 676, 677, 678, 680, 681, 683, 685, 686, 688, 689, 690, 691, 692, 693, 695, 696, 697, 698, 699, 701, 702, 703, 704, 705, 706, 707, 708, 710, 717, 719, 720, 721, 725, 726, 730, 731, 732, 733, 734, 735, 736, 743, 744, 746, 747, 748, 749, 750, 751, 754, 756, 757, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 772, 774, 775, 776, 777, 778, 780, 783, 786, 787, 788, 789, 790, 791, 792, 793, 794, 796, 798, 799, 800, 802, 803, 804, 805, 806, 808, 812, 815, 818, 819, 820, 821, 822, 823, 826, 828, 829, 831, 835, 836, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 874, 875, 876, 877, 878, 879, 880, 881, 883, 884, 885, 886, 887, 888, 889, 890, 891, 897, 898, 899, 901, 902, 903, 904, 906, 907, 908, 909, 910, 912, 915, 917, 918, 919, 923, 924, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 940, 941, 942, 943, 944, 946, 947, 949, 950, 952, 954, 955, 956, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1071, 1074, 1075, 1076, 1077, 1078, 1080, 1081, 1082, 1083, 1088, 1089, 1090, 1091, 1092, 1096, 1097, 1099, 1100, 1101, 1103, 1104, 1106, 1112, 1113, 1114, 1116, 1117, 1118, 1119, 1121, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1135, 1138, 1139, 1140, 1141, 1142, 1143, 1145, 1146, 1147, 1149, 1150, 1152, 1153, 1154, 1155, 1156, 1157, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1192, 1199, 1200, 1201, 1202, 1203, 1205, 1207, 1209, 1210, 1214, 1216, 1217, 1218, 1219, 1220, 1223, 1224, 1225, 1226, 1229, 1230, 1231, 1232, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1251, 1252, 1253, 1255, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1271, 1272, 1273, 1274, 1275, 1278, 1279, 1280, 1281, 1283, 1285, 1286, 1288, 1289, 1290, 1291, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1324, 1325, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1355, 1356, 1357, 1358, 1359, 1360, 1364, 1366, 1368, 1369, 1372, 1373, 1375, 1377, 1378, 1379, 1380, 1382, 1384, 1385, 1388, 1389, 1390, 1391, 1392, 1393, 1395, 1396, 1398, 1401, 1402, 1403, 1404, 1406, 1407, 1408, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1420, 1421, 1422, 1423, 1424, 1429, 1430, 1432, 1433, 1435, 1436, 1437, 1438, 1440, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1468, 1469, 1470, 1471, 1472, 1475, 1476, 1478, 1479, 1480, 1483, 1484, 1485, 1489, 1490, 1491, 1492, 1495, 1498, 1499, 1501, 1504, 1505, 1508, 1510, 1511, 1512, 1513, 1514, 1518, 1520, 1521, 1522, 1524, 1525, 1526, 1527, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1540, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1565, 1566, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1590, 1591, 1592, 1593, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1605, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1618, 1621, 1622, 1623, 1624, 1626, 1627, 1629, 1630, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1642, 1643, 1644, 1645, 1647, 1648, 1649, 1650, 1651, 1652, 1654, 1655, 1656, 1659, 1660, 1661, 1662, 1663, 1664, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1680, 1681, 1682, 1684, 1685, 1686, 1687, 1688, 1691, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1701, 1706, 1709, 1710, 1713, 1716, 1722, 1723, 1725, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1739, 1740, 1741, 1742, 1743, 1745, 1746, 1749, 1751, 1753, 1755, 1756, 1757, 1758, 1759, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1770, 1771, 1775, 1776, 1777, 1779, 1783, 1784, 1785, 1786, 1789, 1790, 1791, 1792, 1793, 1794, 1796, 1797, 1798, 1799, 1800, 1801, 1803, 1804, 1805, 1806, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1818, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1864, 1865, 1866, 1868, 1872, 1873, 1874, 1875, 1877, 1878, 1879, 1880, 1886, 1887, 1888, 1893, 1897, 1900, 1901, 1902, 1904, 1905, 1906, 1915, 1916, 1917, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1932, 1933, 1934, 1935, 1936, 1937, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1949, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1962, 1963, 1964, 1965, 1967, 1968, 1969, 1970, 1971, 1972, 1974, 1975, 1976, 1977, 1979, 1982, 1983, 1985, 1987, 1988, 1989, 1990, 1992, 1994, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2013, 2014, 2015, 2016, 2018, 2019, 2021, 2022, 2024, 2025, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2036, 2037, 2039, 2050, 2051, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2062, 2063, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2083, 2085, 2087, 2088, 2089, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2099, 2101, 2103, 2106, 2108, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2122, 2123, 2124, 2126, 2127, 2128, 2129, 2130, 2133, 2137, 2139, 2140, 2141, 2142, 2144, 2145, 2148, 2150, 2151, 2152, 2153, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2194, 2195, 2196, 2197, 2198, 2199, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2213, 2214, 2216, 2218, 2219, 2220, 2221, 2222, 2223, 2226, 2228, 2229, 2231, 2234, 2235, 2238, 2240, 2241, 2242, 2244, 2249, 2250, 2254, 2257, 2258, 2260, 2261, 2262, 2263, 2265, 2267, 2268, 2269, 2270, 2272, 2273, 2274, 2276, 2278, 2279, 2281, 2283, 2285, 2286, 2287, 2290, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2354, 2355, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2366, 2369, 2371, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2402, 2403, 2404, 2406, 2407, 2408, 2409, 2415, 2417, 2419, 2420, 2421, 2422, 2423, 2425, 2426, 2427, 2428, 2430, 2433, 2434, 2435, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2458, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2469, 2470, 2471, 2472, 2473, 2475, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2486, 2487, 2488, 2489, 2490, 2491, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2508, 2509, 2512, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2525, 2529, 2536, 2537, 2538, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2557, 2558, 2560, 2561, 2563, 2564, 2565, 2566, 2568, 2569, 2570, 2571]}\n",
      "Joe Biden: {'articles': [3, 5, 7, 9, 10, 11, 12, 17, 18, 19, 21, 24, 28, 33, 42, 45, 49, 74, 82, 92, 94, 96, 97, 101, 102, 104, 105, 106, 107, 111, 116, 117, 118, 121, 123, 124, 125, 127, 128, 133, 135, 137, 138, 139, 140, 141, 144, 145, 146, 147, 148, 149, 150, 151, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 220, 234, 245, 246, 262, 265, 266, 269, 271, 273, 278, 279, 280, 285, 288, 291, 292, 295, 296, 297, 301, 302, 304, 306, 308, 310, 311, 313, 316, 320, 321, 322, 326, 328, 329, 330, 331, 332, 333, 335, 337, 343, 344, 346, 348, 350, 351, 352, 355, 358, 359, 362, 364, 366, 369, 370, 371, 372, 374, 382, 387, 390, 396, 397, 399, 405, 409, 411, 413, 420, 427, 428, 429, 430, 433, 437, 438, 439, 440, 441, 445, 446, 448, 449, 458, 460, 461, 466, 472, 473, 474, 476, 477, 479, 482, 485, 487, 491, 492, 493, 494, 495, 496, 498, 499, 515, 522, 528, 534, 546, 548, 552, 553, 557, 560, 562, 563, 565, 568, 570, 574, 576, 578, 579, 580, 583, 584, 585, 586, 588, 591, 592, 593, 594, 595, 596, 597, 598, 599, 601, 602, 603, 604, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 621, 622, 623, 624, 625, 626, 627, 628, 629, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 676, 688, 689, 690, 696, 698, 699, 701, 704, 706, 707, 710, 711, 713, 723, 726, 730, 731, 732, 733, 736, 742, 743, 744, 746, 754, 761, 762, 764, 768, 770, 774, 775, 776, 777, 780, 784, 786, 787, 789, 790, 791, 794, 796, 798, 799, 800, 805, 806, 816, 821, 825, 838, 841, 847, 848, 849, 853, 854, 856, 858, 859, 860, 863, 865, 867, 869, 870, 875, 876, 879, 880, 883, 888, 889, 890, 891, 899, 901, 902, 903, 904, 906, 919, 923, 927, 928, 929, 931, 937, 939, 943, 944, 945, 954, 956, 959, 962, 963, 964, 966, 967, 969, 971, 973, 974, 978, 982, 985, 986, 988, 989, 990, 993, 994, 996, 997, 1000, 1004, 1005, 1006, 1007, 1009, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1081, 1089, 1091, 1099, 1100, 1104, 1106, 1112, 1116, 1119, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1131, 1138, 1141, 1142, 1143, 1147, 1149, 1152, 1153, 1155, 1159, 1160, 1163, 1164, 1165, 1169, 1170, 1171, 1178, 1180, 1181, 1182, 1188, 1189, 1190, 1192, 1193, 1199, 1200, 1201, 1202, 1203, 1205, 1207, 1209, 1214, 1217, 1218, 1220, 1224, 1226, 1228, 1230, 1231, 1233, 1235, 1236, 1237, 1239, 1241, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1251, 1252, 1254, 1255, 1260, 1261, 1262, 1263, 1264, 1266, 1267, 1268, 1269, 1273, 1275, 1278, 1279, 1280, 1281, 1286, 1288, 1289, 1290, 1291, 1294, 1296, 1299, 1300, 1301, 1303, 1304, 1305, 1306, 1307, 1312, 1313, 1314, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1364, 1368, 1369, 1372, 1373, 1377, 1378, 1388, 1389, 1391, 1392, 1393, 1396, 1402, 1404, 1406, 1410, 1411, 1413, 1415, 1419, 1423, 1424, 1430, 1432, 1433, 1435, 1436, 1437, 1438, 1444, 1448, 1449, 1450, 1455, 1457, 1460, 1461, 1462, 1464, 1465, 1466, 1475, 1476, 1478, 1479, 1480, 1482, 1484, 1485, 1486, 1490, 1492, 1497, 1501, 1504, 1508, 1510, 1511, 1516, 1518, 1521, 1522, 1524, 1527, 1531, 1533, 1535, 1536, 1538, 1544, 1545, 1547, 1549, 1550, 1554, 1556, 1557, 1558, 1559, 1562, 1565, 1570, 1571, 1573, 1575, 1577, 1579, 1583, 1584, 1597, 1598, 1599, 1600, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1621, 1622, 1623, 1624, 1626, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1679, 1684, 1686, 1690, 1691, 1693, 1696, 1699, 1702, 1705, 1706, 1707, 1709, 1711, 1716, 1725, 1726, 1727, 1729, 1730, 1731, 1732, 1734, 1740, 1741, 1742, 1751, 1755, 1756, 1758, 1760, 1763, 1764, 1766, 1767, 1768, 1770, 1777, 1783, 1784, 1786, 1789, 1791, 1793, 1797, 1798, 1800, 1801, 1806, 1808, 1809, 1810, 1813, 1814, 1816, 1818, 1820, 1821, 1822, 1825, 1828, 1829, 1831, 1832, 1835, 1837, 1844, 1848, 1850, 1851, 1855, 1858, 1860, 1865, 1868, 1872, 1873, 1878, 1879, 1880, 1886, 1887, 1897, 1901, 1902, 1904, 1905, 1906, 1908, 1909, 1916, 1920, 1922, 1923, 1926, 1927, 1928, 1930, 1931, 1932, 1933, 1934, 1939, 1942, 1943, 1944, 1945, 1946, 1951, 1954, 1955, 1956, 1959, 1962, 1963, 1964, 1969, 1970, 1971, 1976, 1977, 1978, 1979, 1980, 1982, 1983, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2035, 2036, 2037, 2048, 2051, 2053, 2055, 2056, 2057, 2059, 2064, 2072, 2075, 2076, 2077, 2083, 2085, 2087, 2088, 2089, 2091, 2092, 2095, 2096, 2097, 2098, 2099, 2103, 2104, 2107, 2113, 2119, 2122, 2123, 2124, 2126, 2127, 2128, 2132, 2133, 2136, 2139, 2142, 2144, 2145, 2148, 2150, 2151, 2152, 2153, 2157, 2158, 2159, 2160, 2161, 2162, 2168, 2169, 2172, 2173, 2177, 2178, 2179, 2182, 2183, 2186, 2188, 2190, 2192, 2193, 2194, 2195, 2196, 2197, 2199, 2201, 2202, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2213, 2216, 2221, 2222, 2226, 2229, 2231, 2240, 2241, 2244, 2250, 2254, 2257, 2258, 2261, 2263, 2268, 2269, 2271, 2272, 2273, 2274, 2277, 2283, 2287, 2289, 2290, 2293, 2296, 2298, 2300, 2301, 2303, 2305, 2306, 2307, 2310, 2311, 2312, 2313, 2318, 2320, 2321, 2322, 2323, 2326, 2327, 2328, 2330, 2332, 2334, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2366, 2367, 2369, 2370, 2371, 2373, 2374, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2402, 2403, 2404, 2406, 2407, 2408, 2415, 2417, 2420, 2422, 2423, 2427, 2430, 2434, 2437, 2442, 2444, 2446, 2447, 2449, 2452, 2453, 2454, 2456, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2469, 2472, 2473, 2475, 2476, 2479, 2482, 2484, 2486, 2488, 2489, 2490, 2492, 2493, 2494, 2495, 2496, 2497, 2500, 2501, 2502, 2505, 2507, 2508, 2509, 2510, 2511, 2513, 2514, 2515, 2516, 2518, 2520, 2521, 2522, 2525, 2528, 2540, 2541, 2544, 2545, 2546, 2549, 2551, 2552, 2553, 2554, 2558, 2563, 2564, 2565, 2569, 2570, 2571]}\n",
      "Kevin McCarthy: {'articles': [3, 1411, 1189, 1161, 331, 1484, 2124, 366, 622, 560, 1748, 1333, 310, 344, 889, 1818]}\n",
      "Sample edges with weights:\n",
      "Donald Trump - Hillary Clinton: {'weight': 277}\n",
      "Donald Trump - Kamala Harris: {'weight': 1787}\n",
      "Donald Trump - Ruben Gallego: {'weight': 13}\n",
      "Donald Trump - Brian Kemp: {'weight': 13}\n",
      "Donald Trump - Stacey Abrams: {'weight': 9}\n"
     ]
    }
   ],
   "source": [
    "# Create the network graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes with attributes (politicians and articles)\n",
    "for politician, articles in politician_mentions.items():\n",
    "    G.add_node(politician, articles=list(articles))\n",
    "\n",
    "# Add edges based on co-mentions in articles\n",
    "for article_idx, mentioned_politicians in article_mentions.items():\n",
    "    mentioned_politicians = list(mentioned_politicians)\n",
    "    for i in range(len(mentioned_politicians)):\n",
    "        for j in range(i + 1, len(mentioned_politicians)):\n",
    "            p1, p2 = mentioned_politicians[i], mentioned_politicians[j]\n",
    "            if G.has_edge(p1, p2):\n",
    "                G[p1][p2]['weight'] += 1\n",
    "            else:\n",
    "                G.add_edge(p1, p2, weight=1)\n",
    "\n",
    "# Output graph information\n",
    "print(f\"Number of nodes: {G.number_of_nodes()}\")\n",
    "print(f\"Number of edges: {G.number_of_edges()}\")\n",
    "print(\"Sample nodes with attributes:\")\n",
    "for node, attrs in list(G.nodes(data=True))[:5]:\n",
    "    print(f\"{node}: {attrs}\")\n",
    "print(\"Sample edges with weights:\")\n",
    "for u, v, attrs in list(G.edges(data=True))[:5]:\n",
    "    print(f\"{u} - {v}: {attrs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02805SG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
