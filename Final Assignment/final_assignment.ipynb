{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "import datetime\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from newspaper import Article\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize NewsAPI Client\n",
    "api_key = \"\"  # Replace with API key\n",
    "newsapi = NewsApiClient(api_key=api_key)\n",
    "\n",
    "# Input and output file paths\n",
    "input_file = \"news_articles_election_candidates_expanded.csv\"\n",
    "full_content_file = \"news_articles_election_candidates_full_content.csv\"\n",
    "\n",
    "# Function to fetch full article content using newspaper3k\n",
    "def fetch_full_content(article_url):\n",
    "    try:\n",
    "        article = Article(article_url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        return article.text  # Return the full article text\n",
    "    except Exception as e:\n",
    "        return None  # Return None if there is an error\n",
    "\n",
    "# Set new date range\n",
    "start_date = datetime.date(2024, 10, 12)  # Continue from where the previous script left off\n",
    "end_date = datetime.date(2024, 8, 1)  # Adjust end date as needed for backward collection\n",
    "\n",
    "# Prepare to store results\n",
    "articles_data = []\n",
    "\n",
    "# Expanded search queries\n",
    "queries = [\n",
    "    \"2024 Presidential election\",\n",
    "    \"US election AND (Donald Trump OR Kamala Harris)\",\n",
    "    \"Biden administration AND 2024 election\",\n",
    "    \"(Donald Trump OR Trump) AND 2024 election\",\n",
    "    \"(Kamala Harris OR Harris) AND 2024 election\",\n",
    "    \"Campaign financing AND 2024 election\",\n",
    "    \"Voter turnout AND 2024 election\",\n",
    "    \"Presidential debate AND 2024 election\",\n",
    "    \"(Donald Trump OR Trump) AND rally AND 2024\",\n",
    "    \"(Kamala Harris OR Harris) AND speech AND 2024\",\n",
    "    \"(Donald Trump OR Kamala Harris) AND 2024 election\",\n",
    "    \"(Trump OR Harris) AND campaign AND 2024 election\"\n",
    "]\n",
    "\n",
    "# Track API request count to avoid exceeding limits\n",
    "request_count = 0\n",
    "max_requests = 100  # Free-tier daily API limit\n",
    "\n",
    "# Load existing expanded and full content CSV files\n",
    "existing_expanded_data = pd.read_csv(input_file) if os.path.exists(input_file) else pd.DataFrame()\n",
    "existing_full_content_data = pd.read_csv(full_content_file) if os.path.exists(full_content_file) else pd.DataFrame()\n",
    "\n",
    "# Track URLs to avoid duplicates\n",
    "existing_urls = set(existing_expanded_data[\"url\"]) if not existing_expanded_data.empty else set()\n",
    "processed_urls = set(existing_full_content_data[\"url\"]) if not existing_full_content_data.empty else set()\n",
    "\n",
    "# Fetch new articles from NewsAPI\n",
    "current_date = start_date\n",
    "while current_date >= end_date:\n",
    "    # Convert date to string for API\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching articles for {date_str}...\")\n",
    "\n",
    "    for query in queries:\n",
    "        try:\n",
    "            # Check if API limit is reached\n",
    "            if request_count >= max_requests:\n",
    "                print(\"Reached API limit for the day. Exiting script.\")\n",
    "                sys.exit()\n",
    "\n",
    "            # Fetch articles for the current query and date\n",
    "            response = newsapi.get_everything(\n",
    "                q=query,\n",
    "                from_param=date_str,\n",
    "                to=date_str,\n",
    "                language=\"en\",\n",
    "                sort_by=\"relevancy\",  # Fetch relevant articles\n",
    "                page_size=100  # Max articles per API call\n",
    "            )\n",
    "\n",
    "            # Increment request count\n",
    "            request_count += 1\n",
    "\n",
    "            if response.get('status') != 'ok':\n",
    "                print(f\"API error: {response.get('message')}\")\n",
    "                sys.exit()\n",
    "\n",
    "            # Process the articles\n",
    "            if response.get('articles'):\n",
    "                for article in response['articles']:\n",
    "                    # Only add new articles that are not already saved\n",
    "                    if article['url'] not in existing_urls:\n",
    "                        articles_data.append({\n",
    "                            \"query\": query,  # Include the query used for tracking\n",
    "                            \"source\": article['source']['name'],\n",
    "                            \"author\": article['author'],\n",
    "                            \"title\": article['title'],\n",
    "                            \"description\": article['description'],\n",
    "                            \"url\": article['url'],\n",
    "                            \"published_at\": article['publishedAt'],\n",
    "                            \"content\": article['content']\n",
    "                        })\n",
    "                        # Add the URL to the set of existing URLs\n",
    "                        existing_urls.add(article['url'])\n",
    "        except Exception as e:\n",
    "            # Log the error to a file\n",
    "            with open(\"error_log.txt\", \"a\") as log_file:\n",
    "                log_file.write(f\"Error fetching articles for {query} on {date_str}: {e}\\n\")\n",
    "            print(f\"Error fetching articles for {query} on {date_str}: {e}\")\n",
    "    \n",
    "    # Move to the previous day\n",
    "    current_date -= datetime.timedelta(days=1)\n",
    "\n",
    "    # Avoid hitting API limits by adding a small delay between requests\n",
    "    time.sleep(1)\n",
    "\n",
    "# Save new articles to the expanded CSV file\n",
    "if articles_data:\n",
    "    new_data_df = pd.DataFrame(articles_data)\n",
    "    new_data_df.to_csv(input_file, mode='a', header=not os.path.exists(input_file), index=False)\n",
    "    print(f\"Appended {len(new_data_df)} new articles to '{input_file}'.\")\n",
    "\n",
    "# Extract full content for new articles only\n",
    "new_urls = {article[\"url\"] for article in articles_data}  # URLs of newly fetched articles\n",
    "urls_to_process = new_urls - processed_urls  # Exclude already processed URLs\n",
    "\n",
    "if urls_to_process:\n",
    "    full_content_data = []\n",
    "    for url in urls_to_process:\n",
    "        print(f\"Fetching full content for {url}...\")\n",
    "        full_content = fetch_full_content(url)\n",
    "        if full_content:\n",
    "            full_content_data.append({\n",
    "                \"url\": url,\n",
    "                \"full_content\": full_content\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Failed to fetch full content for {url}\")\n",
    "\n",
    "    # Append full content to the full content CSV file\n",
    "    if full_content_data:\n",
    "        full_content_df = pd.DataFrame(full_content_data)\n",
    "        full_content_df.to_csv(full_content_file, mode='a', header=not os.path.exists(full_content_file), index=False)\n",
    "        print(f\"Appended {len(full_content_df)} new full content articles to '{full_content_file}'.\")\n",
    "else:\n",
    "    print(\"No new articles to process for full content.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9610 entries, 0 to 9609\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   query         9610 non-null   object\n",
      " 1   source        9610 non-null   object\n",
      " 2   author        9093 non-null   object\n",
      " 3   title         9607 non-null   object\n",
      " 4   description   9598 non-null   object\n",
      " 5   url           9610 non-null   object\n",
      " 6   published_at  9610 non-null   object\n",
      " 7   content       9610 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 600.8+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "file_path = 'news_articles_election_candidates_expanded.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9610 entries, 0 to 9609\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   query         9610 non-null   object\n",
      " 1   source        9610 non-null   object\n",
      " 2   author        9093 non-null   object\n",
      " 3   title         9607 non-null   object\n",
      " 4   description   9598 non-null   object\n",
      " 5   url           9610 non-null   object\n",
      " 6   published_at  9610 non-null   object\n",
      " 7   content       9610 non-null   object\n",
      " 8   full_content  7556 non-null   object\n",
      "dtypes: object(9)\n",
      "memory usage: 675.8+ KB\n"
     ]
    }
   ],
   "source": [
    "file_path = 'news_articles_election_candidates_full_content.csv'  # Replace with your file path\n",
    "data_full_content = pd.read_csv(file_path)\n",
    "\n",
    "data_full_content.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size: 47.67 MB\n"
     ]
    }
   ],
   "source": [
    "file_size = os.path.getsize('news_articles_election_candidates_full_content.csv') / (1024 * 1024)\n",
    "print(f\"File size: {file_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove entries with missing full content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7556 entries, 0 to 9609\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   query         7556 non-null   object\n",
      " 1   source        7556 non-null   object\n",
      " 2   author        7084 non-null   object\n",
      " 3   title         7553 non-null   object\n",
      " 4   description   7548 non-null   object\n",
      " 5   url           7556 non-null   object\n",
      " 6   published_at  7556 non-null   object\n",
      " 7   content       7556 non-null   object\n",
      " 8   full_content  7556 non-null   object\n",
      "dtypes: object(9)\n",
      "memory usage: 590.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing full content\n",
    "data_full_content = data_full_content.dropna(subset=['full_content'])\n",
    "\n",
    "# Check the updated DataFrame info\n",
    "data_full_content.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned dataframe to a new CSV file\n",
    "data_full_content.to_csv('news_articles_election_candidates_full_content_cleaned.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows (articles): 7556\n",
      "Number of variables (columns): 9\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows (articles):\", data_full_content.shape[0])\n",
    "print(\"Number of variables (columns):\", data_full_content.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique media outlets: 454\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of unique media outlets\n",
    "unique_media_outlets = data_full_content['source'].nunique()\n",
    "\n",
    "# Display the number of unique media outlets\n",
    "print(f\"Number of unique media outlets: {unique_media_outlets}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "02805SG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
